{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Section 2 - Federated Learning - Final Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ewotawa/secure_private_ai/blob/master/Section_2_Federated_Learning_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI8Dfk7ByC6l",
        "colab_type": "text"
      },
      "source": [
        "# Federated Learning Final Project\n",
        "\n",
        "## Overview\n",
        "* See  <a href=\"https://classroom.udacity.com/nanodegrees/nd185/parts/3fe1bb10-68d7-4d84-9c99-9539dedffad5/modules/28d685f0-0cb1-4f94-a8ea-2e16614ab421/lessons/c8fe481d-81ea-41be-8206-06d2deeb8575/concepts/a5fb4b4c-e38a-48de-b2a7-4e853c62acbe\">video</a> for additional details. \n",
        "* Do Federated Learning where the central server is not trusted with the raw gradients.  \n",
        "* In the final project notebook, you'll receive a dataset.  \n",
        "* Train on the dataset using Federated Learning.  \n",
        "* The gradients should not come up to the server in raw form.  \n",
        "* Instead, use the new .move() command to move all of the gradients to one of the workers, sum them up there, and then bring that batch up to the central server and then bring that batch up \n",
        "* Idea: the central server never actually sees the raw gradient for any person.  \n",
        "* We'll look at secure aggregation in course 3.  \n",
        "* For now, do a larger-scale Federated Learning case where you handle the gradients in a special way.\n",
        "\n",
        "## References\n",
        "*  <a href = \"https://blog.openmined.org/upgrade-to-federated-learning-in-10-lines/\">DEEP LEARNING -> FEDERATED LEARNING IN 10 LINES OF PYTORCH + PYSYFT</a>\n",
        "* <a href =\"https://github.com/udacity/private-ai/pull/10\">added data for Federated Learning project</a>\n",
        "* <a href=\"https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%206%20-%20Federated%20Learning%20on%20MNIST%20using%20a%20CNN.ipynb\">Part 6 - Federated Learning on MNIST using a CNN.ipynb</a>\n",
        "* <a href=\"https://docs.google.com/spreadsheets/d/1x-QQK-3Wn86bvSbNTf2_p2FXVCqiic2QwjcArQEuQlg/edit#gid=0\">Slack Channel's reference sheet </a>\n",
        "* <a href=\"https://github.com/ucalyptus/Federated-Learning/blob/master/Federated%20Learning.ipynb\">Federated Learning Example from Slack Channel reference sheet</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmdSUL4X08yg",
        "colab_type": "text"
      },
      "source": [
        "### Install libraries and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9i6x0R11Xtc",
        "colab_type": "code",
        "outputId": "b8644730-4708-406c-d7e1-9787bff1185b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install syft\n",
        "\n",
        "import syft as sy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting syft\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/2e/16bdefc78eb089e1efa9704c33b8f76f035a30dc935bedd7cbb22f6dabaa/syft-0.1.21a1-py3-none-any.whl (219kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.21.2)\n",
            "Collecting tf-encrypted>=0.5.4 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/ce/da9916e7e78f736894b15538b702c0b213fd5d60a7fd6e481d74033a90c0/tf_encrypted-0.5.6-py3-none-manylinux1_x86_64.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Requirement already satisfied: Flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.1)\n",
            "Collecting lz4>=2.1.6 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/c6/96bbb3525a63ebc53ea700cc7d37ab9045542d33b4d262d0f0408ad9bbf2/lz4-2.1.10-cp36-cp36m-manylinux1_x86_64.whl (385kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 37.6MB/s \n",
            "\u001b[?25hCollecting websockets>=7.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/5e/2fe6afbb796c6ac5c006460b5503cd674d33706660337f2dbff10d4aa12d/websockets-8.0-cp36-cp36m-manylinux1_x86_64.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 23.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.16.4)\n",
            "Collecting flask-socketio>=3.3.2 (from syft)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/68/fe4806d3a0a5909d274367eb9b3b87262906c1515024f46c2443a36a0c82/Flask_SocketIO-4.1.0-py2.py3-none-any.whl\n",
            "Collecting msgpack>=0.6.1 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/7e/ae9e91c1bb8d846efafd1f353476e3fd7309778b582d2fb4cea4cc15b9a2/msgpack-0.6.1-cp36-cp36m-manylinux1_x86_64.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 32.5MB/s \n",
            "\u001b[?25hCollecting websocket-client>=0.56.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 43.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tblib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Collecting zstd>=1.4.0.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/27/1ea8086d37424e83ab692015cc8dd7d5e37cf791e339633a40dc828dfb74/zstd-1.4.0.0.tar.gz (450kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 44.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (0.13.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (1.3.0)\n",
            "Requirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Collecting pyyaml>=5.1 (from tf-encrypted>=0.5.4->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/837fefac7475963d1eccf4aa684c23b95aa6c1d033a2c5965ccb11e22623/PyYAML-5.1.1.tar.gz (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (7.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (2.10.1)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (0.15.4)\n",
            "Collecting python-socketio>=2.1.0 (from flask-socketio>=3.3.2->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/1b/57e860a86f2a01be86ae1dacfa0cd8c4dfbfcd4593322268b61b5a07b564/python_socketio-4.2.0-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 19.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.56.0->syft) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->syft) (4.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.1.7)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.33.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=1.0.2->syft) (1.1.1)\n",
            "Collecting python-engineio>=3.8.0 (from python-socketio>=2.1.0->flask-socketio>=3.3.2->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/b8/0fc389ca5c445051b37b17802f80bbf1b51c1e3b48b772ee608efbb90583/python_engineio-3.8.2.post1-py2.py3-none-any.whl (119kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision>=0.3.0->syft) (0.46)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (41.0.1)\n",
            "Building wheels for collected packages: zstd, pyyaml\n",
            "  Building wheel for zstd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/9a/f4/3105b5209674ac77fcca7fede95184c62a95df0196888e0e76\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/27/a1/775c62ddea7bfa62324fd1f65847ed31c55dadb6051481ba3f\n",
            "Successfully built zstd pyyaml\n",
            "Installing collected packages: pyyaml, tf-encrypted, lz4, websockets, python-engineio, python-socketio, flask-socketio, msgpack, websocket-client, zstd, syft\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: msgpack 0.5.6\n",
            "    Uninstalling msgpack-0.5.6:\n",
            "      Successfully uninstalled msgpack-0.5.6\n",
            "Successfully installed flask-socketio-4.1.0 lz4-2.1.10 msgpack-0.6.1 python-engineio-3.8.2.post1 python-socketio-4.2.0 pyyaml-5.1.1 syft-0.1.21a1 tf-encrypted-0.5.6 websocket-client-0.56.0 websockets-8.0 zstd-1.4.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0718 05:29:10.216362 140439781189504 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0718 05:29:10.235803 140439781189504 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJWZI2V2x8gi",
        "colab_type": "code",
        "outputId": "307e2a36-efbf-4ac0-dc6c-daae5f424556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.4)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "928uxHAX1GnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")  # <-- NEW: define remote worker bob\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdUcz5FQQxQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = False\n",
        "        self.seed = 1\n",
        "        self.log_interval = 10\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSvwU0YEBpU2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "04d73e23-d752-47b5-d873-b3ebf6ad0c01"
      },
      "source": [
        "# Note: removed **kwargs from end of federated_train_loader and test_loader definitions.\n",
        "\n",
        "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
        "    batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 8919673.41it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 134630.30it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2217515.14it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 50775.29it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKqyyQItGDSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD6htLEcQ9xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
        "        model.send(data.location) # <-- NEW: send the model to the right location\n",
        "        # data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.get() # <-- NEW: get the model back\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            loss = loss.get() # <-- NEW: get the loss back\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size, #batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_PxFe_fRCNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            # data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_afoMpBxRGnU",
        "colab_type": "code",
        "outputId": "0d6a49d2-1929-4448-a505-423ce57dda10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# model = Net().to(device)\n",
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
        "    test(args, model, device, test_loader)\n",
        "\n",
        "if (args.save_model):\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.305134\n",
            "Train Epoch: 1 [640/60032 (1%)]\tLoss: 2.273475\n",
            "Train Epoch: 1 [1280/60032 (2%)]\tLoss: 2.216174\n",
            "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.156802\n",
            "Train Epoch: 1 [2560/60032 (4%)]\tLoss: 2.139429\n",
            "Train Epoch: 1 [3200/60032 (5%)]\tLoss: 2.053059\n",
            "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 1.896587\n",
            "Train Epoch: 1 [4480/60032 (7%)]\tLoss: 1.917239\n",
            "Train Epoch: 1 [5120/60032 (9%)]\tLoss: 1.655076\n",
            "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 1.440329\n",
            "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 1.231347\n",
            "Train Epoch: 1 [7040/60032 (12%)]\tLoss: 0.983715\n",
            "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 0.867023\n",
            "Train Epoch: 1 [8320/60032 (14%)]\tLoss: 0.890953\n",
            "Train Epoch: 1 [8960/60032 (15%)]\tLoss: 0.861902\n",
            "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.654317\n",
            "Train Epoch: 1 [10240/60032 (17%)]\tLoss: 0.587091\n",
            "Train Epoch: 1 [10880/60032 (18%)]\tLoss: 0.693011\n",
            "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.593099\n",
            "Train Epoch: 1 [12160/60032 (20%)]\tLoss: 0.531986\n",
            "Train Epoch: 1 [12800/60032 (21%)]\tLoss: 0.400264\n",
            "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.455322\n",
            "Train Epoch: 1 [14080/60032 (23%)]\tLoss: 0.438987\n",
            "Train Epoch: 1 [14720/60032 (25%)]\tLoss: 0.398361\n",
            "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.371324\n",
            "Train Epoch: 1 [16000/60032 (27%)]\tLoss: 0.289150\n",
            "Train Epoch: 1 [16640/60032 (28%)]\tLoss: 0.416670\n",
            "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.304617\n",
            "Train Epoch: 1 [17920/60032 (30%)]\tLoss: 0.367789\n",
            "Train Epoch: 1 [18560/60032 (31%)]\tLoss: 0.386265\n",
            "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.314097\n",
            "Train Epoch: 1 [19840/60032 (33%)]\tLoss: 0.238745\n",
            "Train Epoch: 1 [20480/60032 (34%)]\tLoss: 0.534601\n",
            "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 0.369211\n",
            "Train Epoch: 1 [21760/60032 (36%)]\tLoss: 0.464785\n",
            "Train Epoch: 1 [22400/60032 (37%)]\tLoss: 0.279363\n",
            "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.238020\n",
            "Train Epoch: 1 [23680/60032 (39%)]\tLoss: 0.182981\n",
            "Train Epoch: 1 [24320/60032 (41%)]\tLoss: 0.321955\n",
            "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.187752\n",
            "Train Epoch: 1 [25600/60032 (43%)]\tLoss: 0.286318\n",
            "Train Epoch: 1 [26240/60032 (44%)]\tLoss: 0.365828\n",
            "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.523223\n",
            "Train Epoch: 1 [27520/60032 (46%)]\tLoss: 0.147624\n",
            "Train Epoch: 1 [28160/60032 (47%)]\tLoss: 0.131381\n",
            "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.224323\n",
            "Train Epoch: 1 [29440/60032 (49%)]\tLoss: 0.274111\n",
            "Train Epoch: 1 [30080/60032 (50%)]\tLoss: 0.215077\n",
            "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.143916\n",
            "Train Epoch: 1 [31360/60032 (52%)]\tLoss: 0.291434\n",
            "Train Epoch: 1 [32000/60032 (53%)]\tLoss: 0.230507\n",
            "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.268499\n",
            "Train Epoch: 1 [33280/60032 (55%)]\tLoss: 0.205015\n",
            "Train Epoch: 1 [33920/60032 (57%)]\tLoss: 0.147734\n",
            "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.187934\n",
            "Train Epoch: 1 [35200/60032 (59%)]\tLoss: 0.171631\n",
            "Train Epoch: 1 [35840/60032 (60%)]\tLoss: 0.478283\n",
            "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.303621\n",
            "Train Epoch: 1 [37120/60032 (62%)]\tLoss: 0.243553\n",
            "Train Epoch: 1 [37760/60032 (63%)]\tLoss: 0.263034\n",
            "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.239960\n",
            "Train Epoch: 1 [39040/60032 (65%)]\tLoss: 0.456759\n",
            "Train Epoch: 1 [39680/60032 (66%)]\tLoss: 0.200484\n",
            "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 0.256438\n",
            "Train Epoch: 1 [40960/60032 (68%)]\tLoss: 0.196973\n",
            "Train Epoch: 1 [41600/60032 (69%)]\tLoss: 0.270759\n",
            "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.191681\n",
            "Train Epoch: 1 [42880/60032 (71%)]\tLoss: 0.118813\n",
            "Train Epoch: 1 [43520/60032 (72%)]\tLoss: 0.176458\n",
            "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.174667\n",
            "Train Epoch: 1 [44800/60032 (75%)]\tLoss: 0.193119\n",
            "Train Epoch: 1 [45440/60032 (76%)]\tLoss: 0.349294\n",
            "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.220412\n",
            "Train Epoch: 1 [46720/60032 (78%)]\tLoss: 0.191737\n",
            "Train Epoch: 1 [47360/60032 (79%)]\tLoss: 0.155758\n",
            "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.323814\n",
            "Train Epoch: 1 [48640/60032 (81%)]\tLoss: 0.246998\n",
            "Train Epoch: 1 [49280/60032 (82%)]\tLoss: 0.254214\n",
            "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.273938\n",
            "Train Epoch: 1 [50560/60032 (84%)]\tLoss: 0.266370\n",
            "Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.150609\n",
            "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.129327\n",
            "Train Epoch: 1 [52480/60032 (87%)]\tLoss: 0.276623\n",
            "Train Epoch: 1 [53120/60032 (88%)]\tLoss: 0.212820\n",
            "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.182833\n",
            "Train Epoch: 1 [54400/60032 (91%)]\tLoss: 0.143227\n",
            "Train Epoch: 1 [55040/60032 (92%)]\tLoss: 0.118357\n",
            "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.223088\n",
            "Train Epoch: 1 [56320/60032 (94%)]\tLoss: 0.214281\n",
            "Train Epoch: 1 [56960/60032 (95%)]\tLoss: 0.085184\n",
            "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.081163\n",
            "Train Epoch: 1 [58240/60032 (97%)]\tLoss: 0.119667\n",
            "Train Epoch: 1 [58880/60032 (98%)]\tLoss: 0.172809\n",
            "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.143191\n",
            "\n",
            "Test set: Average loss: 0.1575, Accuracy: 9511/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.103221\n",
            "Train Epoch: 2 [640/60032 (1%)]\tLoss: 0.244422\n",
            "Train Epoch: 2 [1280/60032 (2%)]\tLoss: 0.402155\n",
            "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.105845\n",
            "Train Epoch: 2 [2560/60032 (4%)]\tLoss: 0.349342\n",
            "Train Epoch: 2 [3200/60032 (5%)]\tLoss: 0.211654\n",
            "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.147023\n",
            "Train Epoch: 2 [4480/60032 (7%)]\tLoss: 0.192190\n",
            "Train Epoch: 2 [5120/60032 (9%)]\tLoss: 0.103523\n",
            "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.148321\n",
            "Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.148263\n",
            "Train Epoch: 2 [7040/60032 (12%)]\tLoss: 0.060192\n",
            "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.108846\n",
            "Train Epoch: 2 [8320/60032 (14%)]\tLoss: 0.152352\n",
            "Train Epoch: 2 [8960/60032 (15%)]\tLoss: 0.152677\n",
            "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.111557\n",
            "Train Epoch: 2 [10240/60032 (17%)]\tLoss: 0.186860\n",
            "Train Epoch: 2 [10880/60032 (18%)]\tLoss: 0.138993\n",
            "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.118514\n",
            "Train Epoch: 2 [12160/60032 (20%)]\tLoss: 0.221352\n",
            "Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.132464\n",
            "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.063139\n",
            "Train Epoch: 2 [14080/60032 (23%)]\tLoss: 0.124831\n",
            "Train Epoch: 2 [14720/60032 (25%)]\tLoss: 0.163303\n",
            "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.089060\n",
            "Train Epoch: 2 [16000/60032 (27%)]\tLoss: 0.148574\n",
            "Train Epoch: 2 [16640/60032 (28%)]\tLoss: 0.046008\n",
            "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.156941\n",
            "Train Epoch: 2 [17920/60032 (30%)]\tLoss: 0.132319\n",
            "Train Epoch: 2 [18560/60032 (31%)]\tLoss: 0.099179\n",
            "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.159745\n",
            "Train Epoch: 2 [19840/60032 (33%)]\tLoss: 0.054083\n",
            "Train Epoch: 2 [20480/60032 (34%)]\tLoss: 0.210852\n",
            "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.157456\n",
            "Train Epoch: 2 [21760/60032 (36%)]\tLoss: 0.115076\n",
            "Train Epoch: 2 [22400/60032 (37%)]\tLoss: 0.144074\n",
            "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.229732\n",
            "Train Epoch: 2 [23680/60032 (39%)]\tLoss: 0.074357\n",
            "Train Epoch: 2 [24320/60032 (41%)]\tLoss: 0.181132\n",
            "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.197346\n",
            "Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.067677\n",
            "Train Epoch: 2 [26240/60032 (44%)]\tLoss: 0.065908\n",
            "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.206655\n",
            "Train Epoch: 2 [27520/60032 (46%)]\tLoss: 0.115207\n",
            "Train Epoch: 2 [28160/60032 (47%)]\tLoss: 0.153855\n",
            "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.079507\n",
            "Train Epoch: 2 [29440/60032 (49%)]\tLoss: 0.126364\n",
            "Train Epoch: 2 [30080/60032 (50%)]\tLoss: 0.038445\n",
            "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.063148\n",
            "Train Epoch: 2 [31360/60032 (52%)]\tLoss: 0.074466\n",
            "Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.056466\n",
            "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.158469\n",
            "Train Epoch: 2 [33280/60032 (55%)]\tLoss: 0.163727\n",
            "Train Epoch: 2 [33920/60032 (57%)]\tLoss: 0.109543\n",
            "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.156446\n",
            "Train Epoch: 2 [35200/60032 (59%)]\tLoss: 0.072912\n",
            "Train Epoch: 2 [35840/60032 (60%)]\tLoss: 0.130330\n",
            "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.074369\n",
            "Train Epoch: 2 [37120/60032 (62%)]\tLoss: 0.093282\n",
            "Train Epoch: 2 [37760/60032 (63%)]\tLoss: 0.090162\n",
            "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.161415\n",
            "Train Epoch: 2 [39040/60032 (65%)]\tLoss: 0.128906\n",
            "Train Epoch: 2 [39680/60032 (66%)]\tLoss: 0.049100\n",
            "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 0.074256\n",
            "Train Epoch: 2 [40960/60032 (68%)]\tLoss: 0.113933\n",
            "Train Epoch: 2 [41600/60032 (69%)]\tLoss: 0.122479\n",
            "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.153609\n",
            "Train Epoch: 2 [42880/60032 (71%)]\tLoss: 0.138244\n",
            "Train Epoch: 2 [43520/60032 (72%)]\tLoss: 0.107693\n",
            "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.048192\n",
            "Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.102147\n",
            "Train Epoch: 2 [45440/60032 (76%)]\tLoss: 0.116528\n",
            "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.085225\n",
            "Train Epoch: 2 [46720/60032 (78%)]\tLoss: 0.072129\n",
            "Train Epoch: 2 [47360/60032 (79%)]\tLoss: 0.198065\n",
            "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.101033\n",
            "Train Epoch: 2 [48640/60032 (81%)]\tLoss: 0.082122\n",
            "Train Epoch: 2 [49280/60032 (82%)]\tLoss: 0.095091\n",
            "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.154688\n",
            "Train Epoch: 2 [50560/60032 (84%)]\tLoss: 0.106988\n",
            "Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.062860\n",
            "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.032838\n",
            "Train Epoch: 2 [52480/60032 (87%)]\tLoss: 0.167880\n",
            "Train Epoch: 2 [53120/60032 (88%)]\tLoss: 0.069700\n",
            "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.073842\n",
            "Train Epoch: 2 [54400/60032 (91%)]\tLoss: 0.049357\n",
            "Train Epoch: 2 [55040/60032 (92%)]\tLoss: 0.098377\n",
            "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.113492\n",
            "Train Epoch: 2 [56320/60032 (94%)]\tLoss: 0.091218\n",
            "Train Epoch: 2 [56960/60032 (95%)]\tLoss: 0.041372\n",
            "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.111734\n",
            "Train Epoch: 2 [58240/60032 (97%)]\tLoss: 0.083618\n",
            "Train Epoch: 2 [58880/60032 (98%)]\tLoss: 0.064205\n",
            "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.069066\n",
            "\n",
            "Test set: Average loss: 0.0899, Accuracy: 9735/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.081291\n",
            "Train Epoch: 3 [640/60032 (1%)]\tLoss: 0.115016\n",
            "Train Epoch: 3 [1280/60032 (2%)]\tLoss: 0.148862\n",
            "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.080039\n",
            "Train Epoch: 3 [2560/60032 (4%)]\tLoss: 0.141026\n",
            "Train Epoch: 3 [3200/60032 (5%)]\tLoss: 0.090113\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.184774\n",
            "Train Epoch: 3 [4480/60032 (7%)]\tLoss: 0.286939\n",
            "Train Epoch: 3 [5120/60032 (9%)]\tLoss: 0.261452\n",
            "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.072767\n",
            "Train Epoch: 3 [6400/60032 (11%)]\tLoss: 0.129465\n",
            "Train Epoch: 3 [7040/60032 (12%)]\tLoss: 0.095007\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.078837\n",
            "Train Epoch: 3 [8320/60032 (14%)]\tLoss: 0.039845\n",
            "Train Epoch: 3 [8960/60032 (15%)]\tLoss: 0.056576\n",
            "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.206170\n",
            "Train Epoch: 3 [10240/60032 (17%)]\tLoss: 0.121223\n",
            "Train Epoch: 3 [10880/60032 (18%)]\tLoss: 0.132383\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.249777\n",
            "Train Epoch: 3 [12160/60032 (20%)]\tLoss: 0.165334\n",
            "Train Epoch: 3 [12800/60032 (21%)]\tLoss: 0.030447\n",
            "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.121717\n",
            "Train Epoch: 3 [14080/60032 (23%)]\tLoss: 0.089990\n",
            "Train Epoch: 3 [14720/60032 (25%)]\tLoss: 0.171353\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.178415\n",
            "Train Epoch: 3 [16000/60032 (27%)]\tLoss: 0.157047\n",
            "Train Epoch: 3 [16640/60032 (28%)]\tLoss: 0.067125\n",
            "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.150552\n",
            "Train Epoch: 3 [17920/60032 (30%)]\tLoss: 0.075970\n",
            "Train Epoch: 3 [18560/60032 (31%)]\tLoss: 0.017012\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.028152\n",
            "Train Epoch: 3 [19840/60032 (33%)]\tLoss: 0.043230\n",
            "Train Epoch: 3 [20480/60032 (34%)]\tLoss: 0.030028\n",
            "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.046844\n",
            "Train Epoch: 3 [21760/60032 (36%)]\tLoss: 0.119822\n",
            "Train Epoch: 3 [22400/60032 (37%)]\tLoss: 0.148301\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.055311\n",
            "Train Epoch: 3 [23680/60032 (39%)]\tLoss: 0.110982\n",
            "Train Epoch: 3 [24320/60032 (41%)]\tLoss: 0.054740\n",
            "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.034426\n",
            "Train Epoch: 3 [25600/60032 (43%)]\tLoss: 0.204520\n",
            "Train Epoch: 3 [26240/60032 (44%)]\tLoss: 0.036480\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.291176\n",
            "Train Epoch: 3 [27520/60032 (46%)]\tLoss: 0.095636\n",
            "Train Epoch: 3 [28160/60032 (47%)]\tLoss: 0.087709\n",
            "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.055141\n",
            "Train Epoch: 3 [29440/60032 (49%)]\tLoss: 0.157873\n",
            "Train Epoch: 3 [30080/60032 (50%)]\tLoss: 0.095859\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.064245\n",
            "Train Epoch: 3 [31360/60032 (52%)]\tLoss: 0.020321\n",
            "Train Epoch: 3 [32000/60032 (53%)]\tLoss: 0.041318\n",
            "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.025072\n",
            "Train Epoch: 3 [33280/60032 (55%)]\tLoss: 0.051386\n",
            "Train Epoch: 3 [33920/60032 (57%)]\tLoss: 0.058133\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.071311\n",
            "Train Epoch: 3 [35200/60032 (59%)]\tLoss: 0.096919\n",
            "Train Epoch: 3 [35840/60032 (60%)]\tLoss: 0.166712\n",
            "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.039723\n",
            "Train Epoch: 3 [37120/60032 (62%)]\tLoss: 0.143135\n",
            "Train Epoch: 3 [37760/60032 (63%)]\tLoss: 0.028540\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.034051\n",
            "Train Epoch: 3 [39040/60032 (65%)]\tLoss: 0.071771\n",
            "Train Epoch: 3 [39680/60032 (66%)]\tLoss: 0.019140\n",
            "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.082193\n",
            "Train Epoch: 3 [40960/60032 (68%)]\tLoss: 0.018273\n",
            "Train Epoch: 3 [41600/60032 (69%)]\tLoss: 0.118543\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.169696\n",
            "Train Epoch: 3 [42880/60032 (71%)]\tLoss: 0.055946\n",
            "Train Epoch: 3 [43520/60032 (72%)]\tLoss: 0.052105\n",
            "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.054504\n",
            "Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.026263\n",
            "Train Epoch: 3 [45440/60032 (76%)]\tLoss: 0.080822\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.074233\n",
            "Train Epoch: 3 [46720/60032 (78%)]\tLoss: 0.068628\n",
            "Train Epoch: 3 [47360/60032 (79%)]\tLoss: 0.116148\n",
            "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.024500\n",
            "Train Epoch: 3 [48640/60032 (81%)]\tLoss: 0.187242\n",
            "Train Epoch: 3 [49280/60032 (82%)]\tLoss: 0.249473\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.099400\n",
            "Train Epoch: 3 [50560/60032 (84%)]\tLoss: 0.078691\n",
            "Train Epoch: 3 [51200/60032 (85%)]\tLoss: 0.147570\n",
            "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.026502\n",
            "Train Epoch: 3 [52480/60032 (87%)]\tLoss: 0.112970\n",
            "Train Epoch: 3 [53120/60032 (88%)]\tLoss: 0.049495\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.055414\n",
            "Train Epoch: 3 [54400/60032 (91%)]\tLoss: 0.080249\n",
            "Train Epoch: 3 [55040/60032 (92%)]\tLoss: 0.050256\n",
            "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.067579\n",
            "Train Epoch: 3 [56320/60032 (94%)]\tLoss: 0.019180\n",
            "Train Epoch: 3 [56960/60032 (95%)]\tLoss: 0.078065\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.057820\n",
            "Train Epoch: 3 [58240/60032 (97%)]\tLoss: 0.055631\n",
            "Train Epoch: 3 [58880/60032 (98%)]\tLoss: 0.017225\n",
            "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.463102\n",
            "\n",
            "Test set: Average loss: 0.0738, Accuracy: 9758/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.146829\n",
            "Train Epoch: 4 [640/60032 (1%)]\tLoss: 0.140821\n",
            "Train Epoch: 4 [1280/60032 (2%)]\tLoss: 0.027331\n",
            "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.065596\n",
            "Train Epoch: 4 [2560/60032 (4%)]\tLoss: 0.088037\n",
            "Train Epoch: 4 [3200/60032 (5%)]\tLoss: 0.096970\n",
            "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.045302\n",
            "Train Epoch: 4 [4480/60032 (7%)]\tLoss: 0.090846\n",
            "Train Epoch: 4 [5120/60032 (9%)]\tLoss: 0.103507\n",
            "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.101460\n",
            "Train Epoch: 4 [6400/60032 (11%)]\tLoss: 0.018280\n",
            "Train Epoch: 4 [7040/60032 (12%)]\tLoss: 0.076733\n",
            "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.038305\n",
            "Train Epoch: 4 [8320/60032 (14%)]\tLoss: 0.030344\n",
            "Train Epoch: 4 [8960/60032 (15%)]\tLoss: 0.026687\n",
            "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.068318\n",
            "Train Epoch: 4 [10240/60032 (17%)]\tLoss: 0.031968\n",
            "Train Epoch: 4 [10880/60032 (18%)]\tLoss: 0.078717\n",
            "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.116785\n",
            "Train Epoch: 4 [12160/60032 (20%)]\tLoss: 0.107049\n",
            "Train Epoch: 4 [12800/60032 (21%)]\tLoss: 0.033536\n",
            "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.105321\n",
            "Train Epoch: 4 [14080/60032 (23%)]\tLoss: 0.228651\n",
            "Train Epoch: 4 [14720/60032 (25%)]\tLoss: 0.015653\n",
            "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.047861\n",
            "Train Epoch: 4 [16000/60032 (27%)]\tLoss: 0.065255\n",
            "Train Epoch: 4 [16640/60032 (28%)]\tLoss: 0.145419\n",
            "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.048203\n",
            "Train Epoch: 4 [17920/60032 (30%)]\tLoss: 0.038587\n",
            "Train Epoch: 4 [18560/60032 (31%)]\tLoss: 0.045567\n",
            "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.075156\n",
            "Train Epoch: 4 [19840/60032 (33%)]\tLoss: 0.041997\n",
            "Train Epoch: 4 [20480/60032 (34%)]\tLoss: 0.035451\n",
            "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.083796\n",
            "Train Epoch: 4 [21760/60032 (36%)]\tLoss: 0.028792\n",
            "Train Epoch: 4 [22400/60032 (37%)]\tLoss: 0.061869\n",
            "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.024896\n",
            "Train Epoch: 4 [23680/60032 (39%)]\tLoss: 0.112962\n",
            "Train Epoch: 4 [24320/60032 (41%)]\tLoss: 0.088006\n",
            "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.143771\n",
            "Train Epoch: 4 [25600/60032 (43%)]\tLoss: 0.098854\n",
            "Train Epoch: 4 [26240/60032 (44%)]\tLoss: 0.158239\n",
            "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.175964\n",
            "Train Epoch: 4 [27520/60032 (46%)]\tLoss: 0.027592\n",
            "Train Epoch: 4 [28160/60032 (47%)]\tLoss: 0.062299\n",
            "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.093517\n",
            "Train Epoch: 4 [29440/60032 (49%)]\tLoss: 0.153039\n",
            "Train Epoch: 4 [30080/60032 (50%)]\tLoss: 0.024540\n",
            "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.018805\n",
            "Train Epoch: 4 [31360/60032 (52%)]\tLoss: 0.144859\n",
            "Train Epoch: 4 [32000/60032 (53%)]\tLoss: 0.021880\n",
            "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.150391\n",
            "Train Epoch: 4 [33280/60032 (55%)]\tLoss: 0.056411\n",
            "Train Epoch: 4 [33920/60032 (57%)]\tLoss: 0.085769\n",
            "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.029585\n",
            "Train Epoch: 4 [35200/60032 (59%)]\tLoss: 0.065254\n",
            "Train Epoch: 4 [35840/60032 (60%)]\tLoss: 0.097678\n",
            "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.009927\n",
            "Train Epoch: 4 [37120/60032 (62%)]\tLoss: 0.057545\n",
            "Train Epoch: 4 [37760/60032 (63%)]\tLoss: 0.080230\n",
            "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.044069\n",
            "Train Epoch: 4 [39040/60032 (65%)]\tLoss: 0.025078\n",
            "Train Epoch: 4 [39680/60032 (66%)]\tLoss: 0.092041\n",
            "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.049970\n",
            "Train Epoch: 4 [40960/60032 (68%)]\tLoss: 0.012687\n",
            "Train Epoch: 4 [41600/60032 (69%)]\tLoss: 0.133378\n",
            "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.135398\n",
            "Train Epoch: 4 [42880/60032 (71%)]\tLoss: 0.033415\n",
            "Train Epoch: 4 [43520/60032 (72%)]\tLoss: 0.025962\n",
            "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.057950\n",
            "Train Epoch: 4 [44800/60032 (75%)]\tLoss: 0.059852\n",
            "Train Epoch: 4 [45440/60032 (76%)]\tLoss: 0.078188\n",
            "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.020599\n",
            "Train Epoch: 4 [46720/60032 (78%)]\tLoss: 0.044933\n",
            "Train Epoch: 4 [47360/60032 (79%)]\tLoss: 0.068190\n",
            "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.040647\n",
            "Train Epoch: 4 [48640/60032 (81%)]\tLoss: 0.027091\n",
            "Train Epoch: 4 [49280/60032 (82%)]\tLoss: 0.131731\n",
            "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.166192\n",
            "Train Epoch: 4 [50560/60032 (84%)]\tLoss: 0.022503\n",
            "Train Epoch: 4 [51200/60032 (85%)]\tLoss: 0.088655\n",
            "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.067753\n",
            "Train Epoch: 4 [52480/60032 (87%)]\tLoss: 0.016120\n",
            "Train Epoch: 4 [53120/60032 (88%)]\tLoss: 0.076061\n",
            "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.064018\n",
            "Train Epoch: 4 [54400/60032 (91%)]\tLoss: 0.149623\n",
            "Train Epoch: 4 [55040/60032 (92%)]\tLoss: 0.083822\n",
            "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.073588\n",
            "Train Epoch: 4 [56320/60032 (94%)]\tLoss: 0.047142\n",
            "Train Epoch: 4 [56960/60032 (95%)]\tLoss: 0.056050\n",
            "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.061888\n",
            "Train Epoch: 4 [58240/60032 (97%)]\tLoss: 0.024935\n",
            "Train Epoch: 4 [58880/60032 (98%)]\tLoss: 0.056401\n",
            "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.017513\n",
            "\n",
            "Test set: Average loss: 0.0548, Accuracy: 9813/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.082590\n",
            "Train Epoch: 5 [640/60032 (1%)]\tLoss: 0.191632\n",
            "Train Epoch: 5 [1280/60032 (2%)]\tLoss: 0.027748\n",
            "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.040117\n",
            "Train Epoch: 5 [2560/60032 (4%)]\tLoss: 0.027606\n",
            "Train Epoch: 5 [3200/60032 (5%)]\tLoss: 0.013729\n",
            "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.028699\n",
            "Train Epoch: 5 [4480/60032 (7%)]\tLoss: 0.149101\n",
            "Train Epoch: 5 [5120/60032 (9%)]\tLoss: 0.051579\n",
            "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.035378\n",
            "Train Epoch: 5 [6400/60032 (11%)]\tLoss: 0.110916\n",
            "Train Epoch: 5 [7040/60032 (12%)]\tLoss: 0.087541\n",
            "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.063442\n",
            "Train Epoch: 5 [8320/60032 (14%)]\tLoss: 0.069622\n",
            "Train Epoch: 5 [8960/60032 (15%)]\tLoss: 0.153325\n",
            "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.022147\n",
            "Train Epoch: 5 [10240/60032 (17%)]\tLoss: 0.021511\n",
            "Train Epoch: 5 [10880/60032 (18%)]\tLoss: 0.017715\n",
            "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.165380\n",
            "Train Epoch: 5 [12160/60032 (20%)]\tLoss: 0.160860\n",
            "Train Epoch: 5 [12800/60032 (21%)]\tLoss: 0.127810\n",
            "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.046082\n",
            "Train Epoch: 5 [14080/60032 (23%)]\tLoss: 0.027302\n",
            "Train Epoch: 5 [14720/60032 (25%)]\tLoss: 0.065605\n",
            "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.101586\n",
            "Train Epoch: 5 [16000/60032 (27%)]\tLoss: 0.028328\n",
            "Train Epoch: 5 [16640/60032 (28%)]\tLoss: 0.019454\n",
            "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.054320\n",
            "Train Epoch: 5 [17920/60032 (30%)]\tLoss: 0.040218\n",
            "Train Epoch: 5 [18560/60032 (31%)]\tLoss: 0.057851\n",
            "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.034714\n",
            "Train Epoch: 5 [19840/60032 (33%)]\tLoss: 0.036103\n",
            "Train Epoch: 5 [20480/60032 (34%)]\tLoss: 0.031931\n",
            "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.097715\n",
            "Train Epoch: 5 [21760/60032 (36%)]\tLoss: 0.080613\n",
            "Train Epoch: 5 [22400/60032 (37%)]\tLoss: 0.042281\n",
            "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.032395\n",
            "Train Epoch: 5 [23680/60032 (39%)]\tLoss: 0.022558\n",
            "Train Epoch: 5 [24320/60032 (41%)]\tLoss: 0.066808\n",
            "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.026322\n",
            "Train Epoch: 5 [25600/60032 (43%)]\tLoss: 0.083409\n",
            "Train Epoch: 5 [26240/60032 (44%)]\tLoss: 0.151458\n",
            "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.022107\n",
            "Train Epoch: 5 [27520/60032 (46%)]\tLoss: 0.101277\n",
            "Train Epoch: 5 [28160/60032 (47%)]\tLoss: 0.024551\n",
            "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.017720\n",
            "Train Epoch: 5 [29440/60032 (49%)]\tLoss: 0.105302\n",
            "Train Epoch: 5 [30080/60032 (50%)]\tLoss: 0.047151\n",
            "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.039595\n",
            "Train Epoch: 5 [31360/60032 (52%)]\tLoss: 0.053231\n",
            "Train Epoch: 5 [32000/60032 (53%)]\tLoss: 0.094888\n",
            "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.069773\n",
            "Train Epoch: 5 [33280/60032 (55%)]\tLoss: 0.054388\n",
            "Train Epoch: 5 [33920/60032 (57%)]\tLoss: 0.052219\n",
            "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.113465\n",
            "Train Epoch: 5 [35200/60032 (59%)]\tLoss: 0.133883\n",
            "Train Epoch: 5 [35840/60032 (60%)]\tLoss: 0.079842\n",
            "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.037761\n",
            "Train Epoch: 5 [37120/60032 (62%)]\tLoss: 0.046184\n",
            "Train Epoch: 5 [37760/60032 (63%)]\tLoss: 0.020413\n",
            "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.029371\n",
            "Train Epoch: 5 [39040/60032 (65%)]\tLoss: 0.138446\n",
            "Train Epoch: 5 [39680/60032 (66%)]\tLoss: 0.073264\n",
            "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.011653\n",
            "Train Epoch: 5 [40960/60032 (68%)]\tLoss: 0.031342\n",
            "Train Epoch: 5 [41600/60032 (69%)]\tLoss: 0.354484\n",
            "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.026142\n",
            "Train Epoch: 5 [42880/60032 (71%)]\tLoss: 0.032137\n",
            "Train Epoch: 5 [43520/60032 (72%)]\tLoss: 0.022094\n",
            "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.017510\n",
            "Train Epoch: 5 [44800/60032 (75%)]\tLoss: 0.041650\n",
            "Train Epoch: 5 [45440/60032 (76%)]\tLoss: 0.009573\n",
            "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.023011\n",
            "Train Epoch: 5 [46720/60032 (78%)]\tLoss: 0.040824\n",
            "Train Epoch: 5 [47360/60032 (79%)]\tLoss: 0.039383\n",
            "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.015553\n",
            "Train Epoch: 5 [48640/60032 (81%)]\tLoss: 0.009420\n",
            "Train Epoch: 5 [49280/60032 (82%)]\tLoss: 0.040539\n",
            "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.062981\n",
            "Train Epoch: 5 [50560/60032 (84%)]\tLoss: 0.019704\n",
            "Train Epoch: 5 [51200/60032 (85%)]\tLoss: 0.050176\n",
            "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.039655\n",
            "Train Epoch: 5 [52480/60032 (87%)]\tLoss: 0.020420\n",
            "Train Epoch: 5 [53120/60032 (88%)]\tLoss: 0.049979\n",
            "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.016948\n",
            "Train Epoch: 5 [54400/60032 (91%)]\tLoss: 0.032157\n",
            "Train Epoch: 5 [55040/60032 (92%)]\tLoss: 0.063825\n",
            "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.097401\n",
            "Train Epoch: 5 [56320/60032 (94%)]\tLoss: 0.012206\n",
            "Train Epoch: 5 [56960/60032 (95%)]\tLoss: 0.016439\n",
            "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.027165\n",
            "Train Epoch: 5 [58240/60032 (97%)]\tLoss: 0.045493\n",
            "Train Epoch: 5 [58880/60032 (98%)]\tLoss: 0.014956\n",
            "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.061822\n",
            "\n",
            "Test set: Average loss: 0.0460, Accuracy: 9849/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.052845\n",
            "Train Epoch: 6 [640/60032 (1%)]\tLoss: 0.041022\n",
            "Train Epoch: 6 [1280/60032 (2%)]\tLoss: 0.101479\n",
            "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.101472\n",
            "Train Epoch: 6 [2560/60032 (4%)]\tLoss: 0.060263\n",
            "Train Epoch: 6 [3200/60032 (5%)]\tLoss: 0.025262\n",
            "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.011245\n",
            "Train Epoch: 6 [4480/60032 (7%)]\tLoss: 0.032220\n",
            "Train Epoch: 6 [5120/60032 (9%)]\tLoss: 0.063896\n",
            "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.090635\n",
            "Train Epoch: 6 [6400/60032 (11%)]\tLoss: 0.011883\n",
            "Train Epoch: 6 [7040/60032 (12%)]\tLoss: 0.048127\n",
            "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.111328\n",
            "Train Epoch: 6 [8320/60032 (14%)]\tLoss: 0.049169\n",
            "Train Epoch: 6 [8960/60032 (15%)]\tLoss: 0.026860\n",
            "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.014489\n",
            "Train Epoch: 6 [10240/60032 (17%)]\tLoss: 0.082701\n",
            "Train Epoch: 6 [10880/60032 (18%)]\tLoss: 0.014496\n",
            "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.086852\n",
            "Train Epoch: 6 [12160/60032 (20%)]\tLoss: 0.037308\n",
            "Train Epoch: 6 [12800/60032 (21%)]\tLoss: 0.038465\n",
            "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.036762\n",
            "Train Epoch: 6 [14080/60032 (23%)]\tLoss: 0.032877\n",
            "Train Epoch: 6 [14720/60032 (25%)]\tLoss: 0.034338\n",
            "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.097659\n",
            "Train Epoch: 6 [16000/60032 (27%)]\tLoss: 0.065059\n",
            "Train Epoch: 6 [16640/60032 (28%)]\tLoss: 0.072403\n",
            "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.053820\n",
            "Train Epoch: 6 [17920/60032 (30%)]\tLoss: 0.006761\n",
            "Train Epoch: 6 [18560/60032 (31%)]\tLoss: 0.074201\n",
            "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.042853\n",
            "Train Epoch: 6 [19840/60032 (33%)]\tLoss: 0.034724\n",
            "Train Epoch: 6 [20480/60032 (34%)]\tLoss: 0.049317\n",
            "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.073565\n",
            "Train Epoch: 6 [21760/60032 (36%)]\tLoss: 0.084093\n",
            "Train Epoch: 6 [22400/60032 (37%)]\tLoss: 0.114023\n",
            "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.048715\n",
            "Train Epoch: 6 [23680/60032 (39%)]\tLoss: 0.074322\n",
            "Train Epoch: 6 [24320/60032 (41%)]\tLoss: 0.002777\n",
            "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.007704\n",
            "Train Epoch: 6 [25600/60032 (43%)]\tLoss: 0.014966\n",
            "Train Epoch: 6 [26240/60032 (44%)]\tLoss: 0.013714\n",
            "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.019984\n",
            "Train Epoch: 6 [27520/60032 (46%)]\tLoss: 0.044726\n",
            "Train Epoch: 6 [28160/60032 (47%)]\tLoss: 0.051556\n",
            "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.036293\n",
            "Train Epoch: 6 [29440/60032 (49%)]\tLoss: 0.036134\n",
            "Train Epoch: 6 [30080/60032 (50%)]\tLoss: 0.022428\n",
            "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.087016\n",
            "Train Epoch: 6 [31360/60032 (52%)]\tLoss: 0.036529\n",
            "Train Epoch: 6 [32000/60032 (53%)]\tLoss: 0.062339\n",
            "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.010435\n",
            "Train Epoch: 6 [33280/60032 (55%)]\tLoss: 0.037346\n",
            "Train Epoch: 6 [33920/60032 (57%)]\tLoss: 0.031494\n",
            "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.026880\n",
            "Train Epoch: 6 [35200/60032 (59%)]\tLoss: 0.054342\n",
            "Train Epoch: 6 [35840/60032 (60%)]\tLoss: 0.033832\n",
            "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.048298\n",
            "Train Epoch: 6 [37120/60032 (62%)]\tLoss: 0.082996\n",
            "Train Epoch: 6 [37760/60032 (63%)]\tLoss: 0.027112\n",
            "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.006616\n",
            "Train Epoch: 6 [39040/60032 (65%)]\tLoss: 0.046024\n",
            "Train Epoch: 6 [39680/60032 (66%)]\tLoss: 0.024264\n",
            "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.076249\n",
            "Train Epoch: 6 [40960/60032 (68%)]\tLoss: 0.018658\n",
            "Train Epoch: 6 [41600/60032 (69%)]\tLoss: 0.022150\n",
            "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.041769\n",
            "Train Epoch: 6 [42880/60032 (71%)]\tLoss: 0.061550\n",
            "Train Epoch: 6 [43520/60032 (72%)]\tLoss: 0.081969\n",
            "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.007849\n",
            "Train Epoch: 6 [44800/60032 (75%)]\tLoss: 0.118383\n",
            "Train Epoch: 6 [45440/60032 (76%)]\tLoss: 0.005613\n",
            "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.064405\n",
            "Train Epoch: 6 [46720/60032 (78%)]\tLoss: 0.032996\n",
            "Train Epoch: 6 [47360/60032 (79%)]\tLoss: 0.224602\n",
            "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.051831\n",
            "Train Epoch: 6 [48640/60032 (81%)]\tLoss: 0.129895\n",
            "Train Epoch: 6 [49280/60032 (82%)]\tLoss: 0.005333\n",
            "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.024883\n",
            "Train Epoch: 6 [50560/60032 (84%)]\tLoss: 0.005739\n",
            "Train Epoch: 6 [51200/60032 (85%)]\tLoss: 0.019321\n",
            "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.031440\n",
            "Train Epoch: 6 [52480/60032 (87%)]\tLoss: 0.164235\n",
            "Train Epoch: 6 [53120/60032 (88%)]\tLoss: 0.017333\n",
            "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.053846\n",
            "Train Epoch: 6 [54400/60032 (91%)]\tLoss: 0.066497\n",
            "Train Epoch: 6 [55040/60032 (92%)]\tLoss: 0.015297\n",
            "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.024572\n",
            "Train Epoch: 6 [56320/60032 (94%)]\tLoss: 0.021320\n",
            "Train Epoch: 6 [56960/60032 (95%)]\tLoss: 0.107925\n",
            "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.099601\n",
            "Train Epoch: 6 [58240/60032 (97%)]\tLoss: 0.069870\n",
            "Train Epoch: 6 [58880/60032 (98%)]\tLoss: 0.090656\n",
            "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.012264\n",
            "\n",
            "Test set: Average loss: 0.0441, Accuracy: 9859/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.047999\n",
            "Train Epoch: 7 [640/60032 (1%)]\tLoss: 0.034098\n",
            "Train Epoch: 7 [1280/60032 (2%)]\tLoss: 0.104362\n",
            "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.022044\n",
            "Train Epoch: 7 [2560/60032 (4%)]\tLoss: 0.052735\n",
            "Train Epoch: 7 [3200/60032 (5%)]\tLoss: 0.013907\n",
            "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.012739\n",
            "Train Epoch: 7 [4480/60032 (7%)]\tLoss: 0.007364\n",
            "Train Epoch: 7 [5120/60032 (9%)]\tLoss: 0.008462\n",
            "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.040412\n",
            "Train Epoch: 7 [6400/60032 (11%)]\tLoss: 0.051907\n",
            "Train Epoch: 7 [7040/60032 (12%)]\tLoss: 0.045256\n",
            "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.109332\n",
            "Train Epoch: 7 [8320/60032 (14%)]\tLoss: 0.032891\n",
            "Train Epoch: 7 [8960/60032 (15%)]\tLoss: 0.011673\n",
            "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.027032\n",
            "Train Epoch: 7 [10240/60032 (17%)]\tLoss: 0.021082\n",
            "Train Epoch: 7 [10880/60032 (18%)]\tLoss: 0.054524\n",
            "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.075890\n",
            "Train Epoch: 7 [12160/60032 (20%)]\tLoss: 0.028709\n",
            "Train Epoch: 7 [12800/60032 (21%)]\tLoss: 0.071308\n",
            "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.019882\n",
            "Train Epoch: 7 [14080/60032 (23%)]\tLoss: 0.004901\n",
            "Train Epoch: 7 [14720/60032 (25%)]\tLoss: 0.029999\n",
            "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.027513\n",
            "Train Epoch: 7 [16000/60032 (27%)]\tLoss: 0.102506\n",
            "Train Epoch: 7 [16640/60032 (28%)]\tLoss: 0.080353\n",
            "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.163230\n",
            "Train Epoch: 7 [17920/60032 (30%)]\tLoss: 0.015849\n",
            "Train Epoch: 7 [18560/60032 (31%)]\tLoss: 0.030652\n",
            "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.091138\n",
            "Train Epoch: 7 [19840/60032 (33%)]\tLoss: 0.041317\n",
            "Train Epoch: 7 [20480/60032 (34%)]\tLoss: 0.067018\n",
            "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.063134\n",
            "Train Epoch: 7 [21760/60032 (36%)]\tLoss: 0.235911\n",
            "Train Epoch: 7 [22400/60032 (37%)]\tLoss: 0.012818\n",
            "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.014684\n",
            "Train Epoch: 7 [23680/60032 (39%)]\tLoss: 0.021896\n",
            "Train Epoch: 7 [24320/60032 (41%)]\tLoss: 0.013434\n",
            "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.083485\n",
            "Train Epoch: 7 [25600/60032 (43%)]\tLoss: 0.034001\n",
            "Train Epoch: 7 [26240/60032 (44%)]\tLoss: 0.199663\n",
            "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.021518\n",
            "Train Epoch: 7 [27520/60032 (46%)]\tLoss: 0.040660\n",
            "Train Epoch: 7 [28160/60032 (47%)]\tLoss: 0.005133\n",
            "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.012901\n",
            "Train Epoch: 7 [29440/60032 (49%)]\tLoss: 0.009707\n",
            "Train Epoch: 7 [30080/60032 (50%)]\tLoss: 0.065735\n",
            "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.009388\n",
            "Train Epoch: 7 [31360/60032 (52%)]\tLoss: 0.011764\n",
            "Train Epoch: 7 [32000/60032 (53%)]\tLoss: 0.010889\n",
            "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.058238\n",
            "Train Epoch: 7 [33280/60032 (55%)]\tLoss: 0.008698\n",
            "Train Epoch: 7 [33920/60032 (57%)]\tLoss: 0.016098\n",
            "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.024058\n",
            "Train Epoch: 7 [35200/60032 (59%)]\tLoss: 0.034820\n",
            "Train Epoch: 7 [35840/60032 (60%)]\tLoss: 0.140354\n",
            "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.029954\n",
            "Train Epoch: 7 [37120/60032 (62%)]\tLoss: 0.008681\n",
            "Train Epoch: 7 [37760/60032 (63%)]\tLoss: 0.033407\n",
            "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.043035\n",
            "Train Epoch: 7 [39040/60032 (65%)]\tLoss: 0.037521\n",
            "Train Epoch: 7 [39680/60032 (66%)]\tLoss: 0.177179\n",
            "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 0.035829\n",
            "Train Epoch: 7 [40960/60032 (68%)]\tLoss: 0.068473\n",
            "Train Epoch: 7 [41600/60032 (69%)]\tLoss: 0.015905\n",
            "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.012116\n",
            "Train Epoch: 7 [42880/60032 (71%)]\tLoss: 0.005309\n",
            "Train Epoch: 7 [43520/60032 (72%)]\tLoss: 0.009330\n",
            "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.009162\n",
            "Train Epoch: 7 [44800/60032 (75%)]\tLoss: 0.056317\n",
            "Train Epoch: 7 [45440/60032 (76%)]\tLoss: 0.002745\n",
            "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.059630\n",
            "Train Epoch: 7 [46720/60032 (78%)]\tLoss: 0.023628\n",
            "Train Epoch: 7 [47360/60032 (79%)]\tLoss: 0.088340\n",
            "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.044709\n",
            "Train Epoch: 7 [48640/60032 (81%)]\tLoss: 0.131274\n",
            "Train Epoch: 7 [49280/60032 (82%)]\tLoss: 0.030067\n",
            "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.070452\n",
            "Train Epoch: 7 [50560/60032 (84%)]\tLoss: 0.040097\n",
            "Train Epoch: 7 [51200/60032 (85%)]\tLoss: 0.247079\n",
            "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.037277\n",
            "Train Epoch: 7 [52480/60032 (87%)]\tLoss: 0.030579\n",
            "Train Epoch: 7 [53120/60032 (88%)]\tLoss: 0.113283\n",
            "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.033389\n",
            "Train Epoch: 7 [54400/60032 (91%)]\tLoss: 0.022107\n",
            "Train Epoch: 7 [55040/60032 (92%)]\tLoss: 0.044931\n",
            "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.021567\n",
            "Train Epoch: 7 [56320/60032 (94%)]\tLoss: 0.082374\n",
            "Train Epoch: 7 [56960/60032 (95%)]\tLoss: 0.077737\n",
            "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.176802\n",
            "Train Epoch: 7 [58240/60032 (97%)]\tLoss: 0.029829\n",
            "Train Epoch: 7 [58880/60032 (98%)]\tLoss: 0.071788\n",
            "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.026903\n",
            "\n",
            "Test set: Average loss: 0.0446, Accuracy: 9862/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.056659\n",
            "Train Epoch: 8 [640/60032 (1%)]\tLoss: 0.003103\n",
            "Train Epoch: 8 [1280/60032 (2%)]\tLoss: 0.008533\n",
            "Train Epoch: 8 [1920/60032 (3%)]\tLoss: 0.038179\n",
            "Train Epoch: 8 [2560/60032 (4%)]\tLoss: 0.011796\n",
            "Train Epoch: 8 [3200/60032 (5%)]\tLoss: 0.092766\n",
            "Train Epoch: 8 [3840/60032 (6%)]\tLoss: 0.075729\n",
            "Train Epoch: 8 [4480/60032 (7%)]\tLoss: 0.037842\n",
            "Train Epoch: 8 [5120/60032 (9%)]\tLoss: 0.062266\n",
            "Train Epoch: 8 [5760/60032 (10%)]\tLoss: 0.080880\n",
            "Train Epoch: 8 [6400/60032 (11%)]\tLoss: 0.019784\n",
            "Train Epoch: 8 [7040/60032 (12%)]\tLoss: 0.074304\n",
            "Train Epoch: 8 [7680/60032 (13%)]\tLoss: 0.017966\n",
            "Train Epoch: 8 [8320/60032 (14%)]\tLoss: 0.068654\n",
            "Train Epoch: 8 [8960/60032 (15%)]\tLoss: 0.008715\n",
            "Train Epoch: 8 [9600/60032 (16%)]\tLoss: 0.011010\n",
            "Train Epoch: 8 [10240/60032 (17%)]\tLoss: 0.008458\n",
            "Train Epoch: 8 [10880/60032 (18%)]\tLoss: 0.144776\n",
            "Train Epoch: 8 [11520/60032 (19%)]\tLoss: 0.032439\n",
            "Train Epoch: 8 [12160/60032 (20%)]\tLoss: 0.033375\n",
            "Train Epoch: 8 [12800/60032 (21%)]\tLoss: 0.010426\n",
            "Train Epoch: 8 [13440/60032 (22%)]\tLoss: 0.135810\n",
            "Train Epoch: 8 [14080/60032 (23%)]\tLoss: 0.059960\n",
            "Train Epoch: 8 [14720/60032 (25%)]\tLoss: 0.023616\n",
            "Train Epoch: 8 [15360/60032 (26%)]\tLoss: 0.008220\n",
            "Train Epoch: 8 [16000/60032 (27%)]\tLoss: 0.007761\n",
            "Train Epoch: 8 [16640/60032 (28%)]\tLoss: 0.002568\n",
            "Train Epoch: 8 [17280/60032 (29%)]\tLoss: 0.031869\n",
            "Train Epoch: 8 [17920/60032 (30%)]\tLoss: 0.014213\n",
            "Train Epoch: 8 [18560/60032 (31%)]\tLoss: 0.046544\n",
            "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.024362\n",
            "Train Epoch: 8 [19840/60032 (33%)]\tLoss: 0.113858\n",
            "Train Epoch: 8 [20480/60032 (34%)]\tLoss: 0.030146\n",
            "Train Epoch: 8 [21120/60032 (35%)]\tLoss: 0.003008\n",
            "Train Epoch: 8 [21760/60032 (36%)]\tLoss: 0.030363\n",
            "Train Epoch: 8 [22400/60032 (37%)]\tLoss: 0.025760\n",
            "Train Epoch: 8 [23040/60032 (38%)]\tLoss: 0.094172\n",
            "Train Epoch: 8 [23680/60032 (39%)]\tLoss: 0.015257\n",
            "Train Epoch: 8 [24320/60032 (41%)]\tLoss: 0.073744\n",
            "Train Epoch: 8 [24960/60032 (42%)]\tLoss: 0.046612\n",
            "Train Epoch: 8 [25600/60032 (43%)]\tLoss: 0.158675\n",
            "Train Epoch: 8 [26240/60032 (44%)]\tLoss: 0.059743\n",
            "Train Epoch: 8 [26880/60032 (45%)]\tLoss: 0.002846\n",
            "Train Epoch: 8 [27520/60032 (46%)]\tLoss: 0.050142\n",
            "Train Epoch: 8 [28160/60032 (47%)]\tLoss: 0.044632\n",
            "Train Epoch: 8 [28800/60032 (48%)]\tLoss: 0.002485\n",
            "Train Epoch: 8 [29440/60032 (49%)]\tLoss: 0.022649\n",
            "Train Epoch: 8 [30080/60032 (50%)]\tLoss: 0.033663\n",
            "Train Epoch: 8 [30720/60032 (51%)]\tLoss: 0.041518\n",
            "Train Epoch: 8 [31360/60032 (52%)]\tLoss: 0.023581\n",
            "Train Epoch: 8 [32000/60032 (53%)]\tLoss: 0.194519\n",
            "Train Epoch: 8 [32640/60032 (54%)]\tLoss: 0.058854\n",
            "Train Epoch: 8 [33280/60032 (55%)]\tLoss: 0.023630\n",
            "Train Epoch: 8 [33920/60032 (57%)]\tLoss: 0.061561\n",
            "Train Epoch: 8 [34560/60032 (58%)]\tLoss: 0.016703\n",
            "Train Epoch: 8 [35200/60032 (59%)]\tLoss: 0.039166\n",
            "Train Epoch: 8 [35840/60032 (60%)]\tLoss: 0.025113\n",
            "Train Epoch: 8 [36480/60032 (61%)]\tLoss: 0.031350\n",
            "Train Epoch: 8 [37120/60032 (62%)]\tLoss: 0.027913\n",
            "Train Epoch: 8 [37760/60032 (63%)]\tLoss: 0.044543\n",
            "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.068506\n",
            "Train Epoch: 8 [39040/60032 (65%)]\tLoss: 0.008879\n",
            "Train Epoch: 8 [39680/60032 (66%)]\tLoss: 0.012677\n",
            "Train Epoch: 8 [40320/60032 (67%)]\tLoss: 0.045703\n",
            "Train Epoch: 8 [40960/60032 (68%)]\tLoss: 0.115659\n",
            "Train Epoch: 8 [41600/60032 (69%)]\tLoss: 0.040274\n",
            "Train Epoch: 8 [42240/60032 (70%)]\tLoss: 0.046704\n",
            "Train Epoch: 8 [42880/60032 (71%)]\tLoss: 0.002334\n",
            "Train Epoch: 8 [43520/60032 (72%)]\tLoss: 0.051884\n",
            "Train Epoch: 8 [44160/60032 (74%)]\tLoss: 0.019379\n",
            "Train Epoch: 8 [44800/60032 (75%)]\tLoss: 0.146744\n",
            "Train Epoch: 8 [45440/60032 (76%)]\tLoss: 0.019365\n",
            "Train Epoch: 8 [46080/60032 (77%)]\tLoss: 0.055802\n",
            "Train Epoch: 8 [46720/60032 (78%)]\tLoss: 0.148652\n",
            "Train Epoch: 8 [47360/60032 (79%)]\tLoss: 0.007601\n",
            "Train Epoch: 8 [48000/60032 (80%)]\tLoss: 0.028705\n",
            "Train Epoch: 8 [48640/60032 (81%)]\tLoss: 0.114998\n",
            "Train Epoch: 8 [49280/60032 (82%)]\tLoss: 0.004698\n",
            "Train Epoch: 8 [49920/60032 (83%)]\tLoss: 0.014061\n",
            "Train Epoch: 8 [50560/60032 (84%)]\tLoss: 0.043863\n",
            "Train Epoch: 8 [51200/60032 (85%)]\tLoss: 0.021254\n",
            "Train Epoch: 8 [51840/60032 (86%)]\tLoss: 0.041628\n",
            "Train Epoch: 8 [52480/60032 (87%)]\tLoss: 0.063689\n",
            "Train Epoch: 8 [53120/60032 (88%)]\tLoss: 0.003844\n",
            "Train Epoch: 8 [53760/60032 (90%)]\tLoss: 0.037571\n",
            "Train Epoch: 8 [54400/60032 (91%)]\tLoss: 0.011502\n",
            "Train Epoch: 8 [55040/60032 (92%)]\tLoss: 0.019327\n",
            "Train Epoch: 8 [55680/60032 (93%)]\tLoss: 0.010487\n",
            "Train Epoch: 8 [56320/60032 (94%)]\tLoss: 0.036807\n",
            "Train Epoch: 8 [56960/60032 (95%)]\tLoss: 0.004992\n",
            "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.081713\n",
            "Train Epoch: 8 [58240/60032 (97%)]\tLoss: 0.007085\n",
            "Train Epoch: 8 [58880/60032 (98%)]\tLoss: 0.012849\n",
            "Train Epoch: 8 [59520/60032 (99%)]\tLoss: 0.138579\n",
            "\n",
            "Test set: Average loss: 0.0362, Accuracy: 9886/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.005221\n",
            "Train Epoch: 9 [640/60032 (1%)]\tLoss: 0.084376\n",
            "Train Epoch: 9 [1280/60032 (2%)]\tLoss: 0.061507\n",
            "Train Epoch: 9 [1920/60032 (3%)]\tLoss: 0.011011\n",
            "Train Epoch: 9 [2560/60032 (4%)]\tLoss: 0.094140\n",
            "Train Epoch: 9 [3200/60032 (5%)]\tLoss: 0.014418\n",
            "Train Epoch: 9 [3840/60032 (6%)]\tLoss: 0.019707\n",
            "Train Epoch: 9 [4480/60032 (7%)]\tLoss: 0.011745\n",
            "Train Epoch: 9 [5120/60032 (9%)]\tLoss: 0.013157\n",
            "Train Epoch: 9 [5760/60032 (10%)]\tLoss: 0.009392\n",
            "Train Epoch: 9 [6400/60032 (11%)]\tLoss: 0.010081\n",
            "Train Epoch: 9 [7040/60032 (12%)]\tLoss: 0.035735\n",
            "Train Epoch: 9 [7680/60032 (13%)]\tLoss: 0.062374\n",
            "Train Epoch: 9 [8320/60032 (14%)]\tLoss: 0.011431\n",
            "Train Epoch: 9 [8960/60032 (15%)]\tLoss: 0.011509\n",
            "Train Epoch: 9 [9600/60032 (16%)]\tLoss: 0.017745\n",
            "Train Epoch: 9 [10240/60032 (17%)]\tLoss: 0.014124\n",
            "Train Epoch: 9 [10880/60032 (18%)]\tLoss: 0.037441\n",
            "Train Epoch: 9 [11520/60032 (19%)]\tLoss: 0.070276\n",
            "Train Epoch: 9 [12160/60032 (20%)]\tLoss: 0.135452\n",
            "Train Epoch: 9 [12800/60032 (21%)]\tLoss: 0.026995\n",
            "Train Epoch: 9 [13440/60032 (22%)]\tLoss: 0.006879\n",
            "Train Epoch: 9 [14080/60032 (23%)]\tLoss: 0.015284\n",
            "Train Epoch: 9 [14720/60032 (25%)]\tLoss: 0.033568\n",
            "Train Epoch: 9 [15360/60032 (26%)]\tLoss: 0.021353\n",
            "Train Epoch: 9 [16000/60032 (27%)]\tLoss: 0.010064\n",
            "Train Epoch: 9 [16640/60032 (28%)]\tLoss: 0.072461\n",
            "Train Epoch: 9 [17280/60032 (29%)]\tLoss: 0.040855\n",
            "Train Epoch: 9 [17920/60032 (30%)]\tLoss: 0.097195\n",
            "Train Epoch: 9 [18560/60032 (31%)]\tLoss: 0.056922\n",
            "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.022991\n",
            "Train Epoch: 9 [19840/60032 (33%)]\tLoss: 0.001831\n",
            "Train Epoch: 9 [20480/60032 (34%)]\tLoss: 0.011748\n",
            "Train Epoch: 9 [21120/60032 (35%)]\tLoss: 0.007182\n",
            "Train Epoch: 9 [21760/60032 (36%)]\tLoss: 0.009269\n",
            "Train Epoch: 9 [22400/60032 (37%)]\tLoss: 0.074973\n",
            "Train Epoch: 9 [23040/60032 (38%)]\tLoss: 0.071696\n",
            "Train Epoch: 9 [23680/60032 (39%)]\tLoss: 0.016536\n",
            "Train Epoch: 9 [24320/60032 (41%)]\tLoss: 0.010128\n",
            "Train Epoch: 9 [24960/60032 (42%)]\tLoss: 0.030944\n",
            "Train Epoch: 9 [25600/60032 (43%)]\tLoss: 0.034434\n",
            "Train Epoch: 9 [26240/60032 (44%)]\tLoss: 0.023989\n",
            "Train Epoch: 9 [26880/60032 (45%)]\tLoss: 0.034639\n",
            "Train Epoch: 9 [27520/60032 (46%)]\tLoss: 0.069216\n",
            "Train Epoch: 9 [28160/60032 (47%)]\tLoss: 0.026672\n",
            "Train Epoch: 9 [28800/60032 (48%)]\tLoss: 0.127856\n",
            "Train Epoch: 9 [29440/60032 (49%)]\tLoss: 0.015918\n",
            "Train Epoch: 9 [30080/60032 (50%)]\tLoss: 0.003149\n",
            "Train Epoch: 9 [30720/60032 (51%)]\tLoss: 0.044178\n",
            "Train Epoch: 9 [31360/60032 (52%)]\tLoss: 0.008925\n",
            "Train Epoch: 9 [32000/60032 (53%)]\tLoss: 0.002760\n",
            "Train Epoch: 9 [32640/60032 (54%)]\tLoss: 0.049925\n",
            "Train Epoch: 9 [33280/60032 (55%)]\tLoss: 0.097224\n",
            "Train Epoch: 9 [33920/60032 (57%)]\tLoss: 0.011822\n",
            "Train Epoch: 9 [34560/60032 (58%)]\tLoss: 0.028268\n",
            "Train Epoch: 9 [35200/60032 (59%)]\tLoss: 0.004451\n",
            "Train Epoch: 9 [35840/60032 (60%)]\tLoss: 0.029053\n",
            "Train Epoch: 9 [36480/60032 (61%)]\tLoss: 0.034851\n",
            "Train Epoch: 9 [37120/60032 (62%)]\tLoss: 0.042928\n",
            "Train Epoch: 9 [37760/60032 (63%)]\tLoss: 0.007711\n",
            "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.010666\n",
            "Train Epoch: 9 [39040/60032 (65%)]\tLoss: 0.088333\n",
            "Train Epoch: 9 [39680/60032 (66%)]\tLoss: 0.005372\n",
            "Train Epoch: 9 [40320/60032 (67%)]\tLoss: 0.021169\n",
            "Train Epoch: 9 [40960/60032 (68%)]\tLoss: 0.024116\n",
            "Train Epoch: 9 [41600/60032 (69%)]\tLoss: 0.007750\n",
            "Train Epoch: 9 [42240/60032 (70%)]\tLoss: 0.031291\n",
            "Train Epoch: 9 [42880/60032 (71%)]\tLoss: 0.022429\n",
            "Train Epoch: 9 [43520/60032 (72%)]\tLoss: 0.037185\n",
            "Train Epoch: 9 [44160/60032 (74%)]\tLoss: 0.004419\n",
            "Train Epoch: 9 [44800/60032 (75%)]\tLoss: 0.006216\n",
            "Train Epoch: 9 [45440/60032 (76%)]\tLoss: 0.030606\n",
            "Train Epoch: 9 [46080/60032 (77%)]\tLoss: 0.049544\n",
            "Train Epoch: 9 [46720/60032 (78%)]\tLoss: 0.009908\n",
            "Train Epoch: 9 [47360/60032 (79%)]\tLoss: 0.156226\n",
            "Train Epoch: 9 [48000/60032 (80%)]\tLoss: 0.013750\n",
            "Train Epoch: 9 [48640/60032 (81%)]\tLoss: 0.026456\n",
            "Train Epoch: 9 [49280/60032 (82%)]\tLoss: 0.033159\n",
            "Train Epoch: 9 [49920/60032 (83%)]\tLoss: 0.006207\n",
            "Train Epoch: 9 [50560/60032 (84%)]\tLoss: 0.010933\n",
            "Train Epoch: 9 [51200/60032 (85%)]\tLoss: 0.014837\n",
            "Train Epoch: 9 [51840/60032 (86%)]\tLoss: 0.072524\n",
            "Train Epoch: 9 [52480/60032 (87%)]\tLoss: 0.020404\n",
            "Train Epoch: 9 [53120/60032 (88%)]\tLoss: 0.023534\n",
            "Train Epoch: 9 [53760/60032 (90%)]\tLoss: 0.019514\n",
            "Train Epoch: 9 [54400/60032 (91%)]\tLoss: 0.017296\n",
            "Train Epoch: 9 [55040/60032 (92%)]\tLoss: 0.013314\n",
            "Train Epoch: 9 [55680/60032 (93%)]\tLoss: 0.006574\n",
            "Train Epoch: 9 [56320/60032 (94%)]\tLoss: 0.074066\n",
            "Train Epoch: 9 [56960/60032 (95%)]\tLoss: 0.084104\n",
            "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.010870\n",
            "Train Epoch: 9 [58240/60032 (97%)]\tLoss: 0.006524\n",
            "Train Epoch: 9 [58880/60032 (98%)]\tLoss: 0.039178\n",
            "Train Epoch: 9 [59520/60032 (99%)]\tLoss: 0.006324\n",
            "\n",
            "Test set: Average loss: 0.0344, Accuracy: 9891/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.004203\n",
            "Train Epoch: 10 [640/60032 (1%)]\tLoss: 0.009176\n",
            "Train Epoch: 10 [1280/60032 (2%)]\tLoss: 0.011273\n",
            "Train Epoch: 10 [1920/60032 (3%)]\tLoss: 0.030270\n",
            "Train Epoch: 10 [2560/60032 (4%)]\tLoss: 0.004841\n",
            "Train Epoch: 10 [3200/60032 (5%)]\tLoss: 0.009510\n",
            "Train Epoch: 10 [3840/60032 (6%)]\tLoss: 0.029526\n",
            "Train Epoch: 10 [4480/60032 (7%)]\tLoss: 0.009872\n",
            "Train Epoch: 10 [5120/60032 (9%)]\tLoss: 0.020855\n",
            "Train Epoch: 10 [5760/60032 (10%)]\tLoss: 0.002077\n",
            "Train Epoch: 10 [6400/60032 (11%)]\tLoss: 0.056733\n",
            "Train Epoch: 10 [7040/60032 (12%)]\tLoss: 0.006966\n",
            "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.088129\n",
            "Train Epoch: 10 [8320/60032 (14%)]\tLoss: 0.049797\n",
            "Train Epoch: 10 [8960/60032 (15%)]\tLoss: 0.031724\n",
            "Train Epoch: 10 [9600/60032 (16%)]\tLoss: 0.031981\n",
            "Train Epoch: 10 [10240/60032 (17%)]\tLoss: 0.037559\n",
            "Train Epoch: 10 [10880/60032 (18%)]\tLoss: 0.004699\n",
            "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.037131\n",
            "Train Epoch: 10 [12160/60032 (20%)]\tLoss: 0.007529\n",
            "Train Epoch: 10 [12800/60032 (21%)]\tLoss: 0.002339\n",
            "Train Epoch: 10 [13440/60032 (22%)]\tLoss: 0.009185\n",
            "Train Epoch: 10 [14080/60032 (23%)]\tLoss: 0.018396\n",
            "Train Epoch: 10 [14720/60032 (25%)]\tLoss: 0.029989\n",
            "Train Epoch: 10 [15360/60032 (26%)]\tLoss: 0.002562\n",
            "Train Epoch: 10 [16000/60032 (27%)]\tLoss: 0.057425\n",
            "Train Epoch: 10 [16640/60032 (28%)]\tLoss: 0.103454\n",
            "Train Epoch: 10 [17280/60032 (29%)]\tLoss: 0.035339\n",
            "Train Epoch: 10 [17920/60032 (30%)]\tLoss: 0.014449\n",
            "Train Epoch: 10 [18560/60032 (31%)]\tLoss: 0.017456\n",
            "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.146149\n",
            "Train Epoch: 10 [19840/60032 (33%)]\tLoss: 0.022699\n",
            "Train Epoch: 10 [20480/60032 (34%)]\tLoss: 0.006262\n",
            "Train Epoch: 10 [21120/60032 (35%)]\tLoss: 0.034385\n",
            "Train Epoch: 10 [21760/60032 (36%)]\tLoss: 0.018905\n",
            "Train Epoch: 10 [22400/60032 (37%)]\tLoss: 0.003387\n",
            "Train Epoch: 10 [23040/60032 (38%)]\tLoss: 0.189797\n",
            "Train Epoch: 10 [23680/60032 (39%)]\tLoss: 0.018028\n",
            "Train Epoch: 10 [24320/60032 (41%)]\tLoss: 0.015050\n",
            "Train Epoch: 10 [24960/60032 (42%)]\tLoss: 0.009782\n",
            "Train Epoch: 10 [25600/60032 (43%)]\tLoss: 0.023960\n",
            "Train Epoch: 10 [26240/60032 (44%)]\tLoss: 0.011002\n",
            "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.012849\n",
            "Train Epoch: 10 [27520/60032 (46%)]\tLoss: 0.007212\n",
            "Train Epoch: 10 [28160/60032 (47%)]\tLoss: 0.026730\n",
            "Train Epoch: 10 [28800/60032 (48%)]\tLoss: 0.008088\n",
            "Train Epoch: 10 [29440/60032 (49%)]\tLoss: 0.011871\n",
            "Train Epoch: 10 [30080/60032 (50%)]\tLoss: 0.081546\n",
            "Train Epoch: 10 [30720/60032 (51%)]\tLoss: 0.013652\n",
            "Train Epoch: 10 [31360/60032 (52%)]\tLoss: 0.013216\n",
            "Train Epoch: 10 [32000/60032 (53%)]\tLoss: 0.022778\n",
            "Train Epoch: 10 [32640/60032 (54%)]\tLoss: 0.027806\n",
            "Train Epoch: 10 [33280/60032 (55%)]\tLoss: 0.096831\n",
            "Train Epoch: 10 [33920/60032 (57%)]\tLoss: 0.010408\n",
            "Train Epoch: 10 [34560/60032 (58%)]\tLoss: 0.033577\n",
            "Train Epoch: 10 [35200/60032 (59%)]\tLoss: 0.048404\n",
            "Train Epoch: 10 [35840/60032 (60%)]\tLoss: 0.005352\n",
            "Train Epoch: 10 [36480/60032 (61%)]\tLoss: 0.017015\n",
            "Train Epoch: 10 [37120/60032 (62%)]\tLoss: 0.009835\n",
            "Train Epoch: 10 [37760/60032 (63%)]\tLoss: 0.112933\n",
            "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.028350\n",
            "Train Epoch: 10 [39040/60032 (65%)]\tLoss: 0.041417\n",
            "Train Epoch: 10 [39680/60032 (66%)]\tLoss: 0.030024\n",
            "Train Epoch: 10 [40320/60032 (67%)]\tLoss: 0.078473\n",
            "Train Epoch: 10 [40960/60032 (68%)]\tLoss: 0.084505\n",
            "Train Epoch: 10 [41600/60032 (69%)]\tLoss: 0.057242\n",
            "Train Epoch: 10 [42240/60032 (70%)]\tLoss: 0.014510\n",
            "Train Epoch: 10 [42880/60032 (71%)]\tLoss: 0.037008\n",
            "Train Epoch: 10 [43520/60032 (72%)]\tLoss: 0.016122\n",
            "Train Epoch: 10 [44160/60032 (74%)]\tLoss: 0.004908\n",
            "Train Epoch: 10 [44800/60032 (75%)]\tLoss: 0.068341\n",
            "Train Epoch: 10 [45440/60032 (76%)]\tLoss: 0.015999\n",
            "Train Epoch: 10 [46080/60032 (77%)]\tLoss: 0.004810\n",
            "Train Epoch: 10 [46720/60032 (78%)]\tLoss: 0.172246\n",
            "Train Epoch: 10 [47360/60032 (79%)]\tLoss: 0.007947\n",
            "Train Epoch: 10 [48000/60032 (80%)]\tLoss: 0.076611\n",
            "Train Epoch: 10 [48640/60032 (81%)]\tLoss: 0.010773\n",
            "Train Epoch: 10 [49280/60032 (82%)]\tLoss: 0.071406\n",
            "Train Epoch: 10 [49920/60032 (83%)]\tLoss: 0.043203\n",
            "Train Epoch: 10 [50560/60032 (84%)]\tLoss: 0.191546\n",
            "Train Epoch: 10 [51200/60032 (85%)]\tLoss: 0.008314\n",
            "Train Epoch: 10 [51840/60032 (86%)]\tLoss: 0.004271\n",
            "Train Epoch: 10 [52480/60032 (87%)]\tLoss: 0.009764\n",
            "Train Epoch: 10 [53120/60032 (88%)]\tLoss: 0.018046\n",
            "Train Epoch: 10 [53760/60032 (90%)]\tLoss: 0.100702\n",
            "Train Epoch: 10 [54400/60032 (91%)]\tLoss: 0.061744\n",
            "Train Epoch: 10 [55040/60032 (92%)]\tLoss: 0.051636\n",
            "Train Epoch: 10 [55680/60032 (93%)]\tLoss: 0.003043\n",
            "Train Epoch: 10 [56320/60032 (94%)]\tLoss: 0.026447\n",
            "Train Epoch: 10 [56960/60032 (95%)]\tLoss: 0.033198\n",
            "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.013565\n",
            "Train Epoch: 10 [58240/60032 (97%)]\tLoss: 0.048527\n",
            "Train Epoch: 10 [58880/60032 (98%)]\tLoss: 0.107387\n",
            "Train Epoch: 10 [59520/60032 (99%)]\tLoss: 0.007521\n",
            "\n",
            "Test set: Average loss: 0.0376, Accuracy: 9872/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}