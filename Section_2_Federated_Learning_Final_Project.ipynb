{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Section 2 - Federated Learning - Final Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ewotawa/secure_private_ai/blob/master/Section_2_Federated_Learning_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI8Dfk7ByC6l",
        "colab_type": "text"
      },
      "source": [
        "# Federated Learning Final Project\n",
        "\n",
        "## Overview\n",
        "* See  <a href=\"https://classroom.udacity.com/nanodegrees/nd185/parts/3fe1bb10-68d7-4d84-9c99-9539dedffad5/modules/28d685f0-0cb1-4f94-a8ea-2e16614ab421/lessons/c8fe481d-81ea-41be-8206-06d2deeb8575/concepts/a5fb4b4c-e38a-48de-b2a7-4e853c62acbe\">video</a> for additional details. \n",
        "* Do Federated Learning where the central server is not trusted with the raw gradients.  \n",
        "* In the final project notebook, you'll receive a dataset.  \n",
        "* Train on the dataset using Federated Learning.  \n",
        "* The gradients should not come up to the server in raw form.  \n",
        "* Instead, use the new .move() command to move all of the gradients to one of the workers, sum them up there, and then bring that batch up to the central server and then bring that batch up \n",
        "* Idea: the central server never actually sees the raw gradient for any person.  \n",
        "* We'll look at secure aggregation in course 3.  \n",
        "* For now, do a larger-scale Federated Learning case where you handle the gradients in a special way.\n",
        "\n",
        "## Approach\n",
        "* Use the method illustrated in the \"DEEP LEARNING\" article referenced below. Update the code such that the MNIST model trains locally. Updated for my personal code style preferences.\n",
        "* Per conversation in the SPAIC Slack channel, use of a federated data loader approach trains the model and keeps the disaggregated gradients off of the local machine. The aggregate model returns when model.get() is called.\n",
        "* Contacted the team at OpenMined. They confirmed that PySyft currently does not work with GPUs, although updates are in progress. (7/18/2019).\n",
        "\n",
        "## References\n",
        "*  <a href = \"https://blog.openmined.org/upgrade-to-federated-learning-in-10-lines/\">DEEP LEARNING -> FEDERATED LEARNING IN 10 LINES OF PYTORCH + PYSYFT</a>\n",
        "* <a href =\"https://github.com/udacity/private-ai/pull/10\">added data for Federated Learning project</a>\n",
        "* <a href=\"https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%206%20-%20Federated%20Learning%20on%20MNIST%20using%20a%20CNN.ipynb\">Part 6 - Federated Learning on MNIST using a CNN.ipynb</a>\n",
        "* <a href=\"https://docs.google.com/spreadsheets/d/1x-QQK-3Wn86bvSbNTf2_p2FXVCqiic2QwjcArQEuQlg/edit#gid=0\">Slack Channel's reference sheet </a>\n",
        "* <a href=\"https://github.com/ucalyptus/Federated-Learning/blob/master/Federated%20Learning.ipynb\">Federated Learning Example from Slack Channel reference sheet</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmdSUL4X08yg",
        "colab_type": "text"
      },
      "source": [
        "### Install libraries and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9i6x0R11Xtc",
        "colab_type": "code",
        "outputId": "ddc97777-8acd-47b6-fd61-da8adf43b3d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install syft\n",
        "\n",
        "import syft as sy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting syft\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/2e/16bdefc78eb089e1efa9704c33b8f76f035a30dc935bedd7cbb22f6dabaa/syft-0.1.21a1-py3-none-any.whl (219kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 2.8MB/s \n",
            "\u001b[?25hCollecting zstd>=1.4.0.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/27/1ea8086d37424e83ab692015cc8dd7d5e37cf791e339633a40dc828dfb74/zstd-1.4.0.0.tar.gz (450kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 28.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.1)\n",
            "Collecting msgpack>=0.6.1 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/7e/ae9e91c1bb8d846efafd1f353476e3fd7309778b582d2fb4cea4cc15b9a2/msgpack-0.6.1-cp36-cp36m-manylinux1_x86_64.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 41.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tblib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Collecting flask-socketio>=3.3.2 (from syft)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/68/fe4806d3a0a5909d274367eb9b3b87262906c1515024f46c2443a36a0c82/Flask_SocketIO-4.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.21.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.16.4)\n",
            "Collecting lz4>=2.1.6 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/c6/96bbb3525a63ebc53ea700cc7d37ab9045542d33b4d262d0f0408ad9bbf2/lz4-2.1.10-cp36-cp36m-manylinux1_x86_64.whl (385kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 41.2MB/s \n",
            "\u001b[?25hCollecting tf-encrypted>=0.5.4 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/ff/7dbd5fc77fcec0df1798268a6b72a2ab0150b854761bc39c77d566798f0b/tf_encrypted-0.5.7-py3-none-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 35.3MB/s \n",
            "\u001b[?25hCollecting websocket-client>=0.56.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 41.8MB/s \n",
            "\u001b[?25hCollecting websockets>=7.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/5e/2fe6afbb796c6ac5c006460b5503cd674d33706660337f2dbff10d4aa12d/websockets-8.0-cp36-cp36m-manylinux1_x86_64.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 27.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (2.10.1)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (0.15.4)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (7.0)\n",
            "Collecting python-socketio>=2.1.0 (from flask-socketio>=3.3.2->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/1b/57e860a86f2a01be86ae1dacfa0cd8c4dfbfcd4593322268b61b5a07b564/python_socketio-4.2.0-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 19.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->syft) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->syft) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (0.13.2)\n",
            "Collecting pyyaml>=5.1 (from tf-encrypted>=0.5.4->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/837fefac7475963d1eccf4aa684c23b95aa6c1d033a2c5965ccb11e22623/PyYAML-5.1.1.tar.gz (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 21.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=1.0.2->syft) (1.1.1)\n",
            "Collecting python-engineio>=3.8.0 (from python-socketio>=2.1.0->flask-socketio>=3.3.2->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/b8/0fc389ca5c445051b37b17802f80bbf1b51c1e3b48b772ee608efbb90583/python_engineio-3.8.2.post1-py2.py3-none-any.whl (119kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 39.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision>=0.3.0->syft) (0.46)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.1.7)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.33.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.11.2)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.1.1)\n",
            "Building wheels for collected packages: zstd, pyyaml\n",
            "  Building wheel for zstd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/9a/f4/3105b5209674ac77fcca7fede95184c62a95df0196888e0e76\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/27/a1/775c62ddea7bfa62324fd1f65847ed31c55dadb6051481ba3f\n",
            "Successfully built zstd pyyaml\n",
            "Installing collected packages: zstd, msgpack, python-engineio, python-socketio, flask-socketio, lz4, pyyaml, tf-encrypted, websocket-client, websockets, syft\n",
            "  Found existing installation: msgpack 0.5.6\n",
            "    Uninstalling msgpack-0.5.6:\n",
            "      Successfully uninstalled msgpack-0.5.6\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed flask-socketio-4.1.0 lz4-2.1.10 msgpack-0.6.1 python-engineio-3.8.2.post1 python-socketio-4.2.0 pyyaml-5.1.1 syft-0.1.21a1 tf-encrypted-0.5.7 websocket-client-0.56.0 websockets-8.0 zstd-1.4.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0719 05:33:27.103885 140427994863488 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0719 05:33:27.124652 140427994863488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJWZI2V2x8gi",
        "colab_type": "code",
        "outputId": "8d2ebbe0-9aeb-4fde-eed8-63aa356096c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.4)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "928uxHAX1GnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
        "vw00 = sy.VirtualWorker(hook, id=\"vw00\")\n",
        "vw01 = sy.VirtualWorker(hook, id=\"vw01\")\n",
        "\n",
        "aggr = sy.VirtualWorker(hook, id=\"aggr\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdUcz5FQQxQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = False\n",
        "        self.seed = 1\n",
        "        self.log_interval = 10\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSvwU0YEBpU2",
        "colab_type": "code",
        "outputId": "bf293237-1d00-42d7-b2cd-3a757a017eb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Note: removed **kwargs from end of federated_train_loader and test_loader definitions.\n",
        "\n",
        "transform = transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])\n",
        "\n",
        "federated_train_loader = sy.FederatedDataLoader(datasets.MNIST('../data', train=True, download=True, transform=transform).federate((vw00, vw01)), \n",
        "                                                batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False, transform=transform), \n",
        "                                          batch_size=args.test_batch_size, shuffle=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 3842234.09it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 57197.76it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 957260.38it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 21524.23it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKqyyQItGDSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD6htLEcQ9xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
        "        model.send(data.location) # <-- NEW: send the model to the right location\n",
        "        # data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.get() # <-- NEW: get the model back\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            loss = loss.get() # <-- NEW: get the loss back\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size, #batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_PxFe_fRCNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            # data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_afoMpBxRGnU",
        "colab_type": "code",
        "outputId": "9d61b84f-680b-42ea-c71b-7c39fbad16db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# model = Net().to(device)\n",
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
        "    test(args, model, device, test_loader)\n",
        "\n",
        "if (args.save_model):\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.305818\n",
            "Train Epoch: 1 [640/60032 (1%)]\tLoss: 2.281389\n",
            "Train Epoch: 1 [1280/60032 (2%)]\tLoss: 2.249339\n",
            "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.207941\n",
            "Train Epoch: 1 [2560/60032 (4%)]\tLoss: 2.154252\n",
            "Train Epoch: 1 [3200/60032 (5%)]\tLoss: 2.107017\n",
            "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 2.000264\n",
            "Train Epoch: 1 [4480/60032 (7%)]\tLoss: 1.924190\n",
            "Train Epoch: 1 [5120/60032 (9%)]\tLoss: 1.751883\n",
            "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 1.495690\n",
            "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 1.385576\n",
            "Train Epoch: 1 [7040/60032 (12%)]\tLoss: 1.188629\n",
            "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 1.049302\n",
            "Train Epoch: 1 [8320/60032 (14%)]\tLoss: 0.858565\n",
            "Train Epoch: 1 [8960/60032 (15%)]\tLoss: 0.622664\n",
            "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.820056\n",
            "Train Epoch: 1 [10240/60032 (17%)]\tLoss: 0.529858\n",
            "Train Epoch: 1 [10880/60032 (18%)]\tLoss: 0.546309\n",
            "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.677349\n",
            "Train Epoch: 1 [12160/60032 (20%)]\tLoss: 0.372410\n",
            "Train Epoch: 1 [12800/60032 (21%)]\tLoss: 0.512144\n",
            "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.506294\n",
            "Train Epoch: 1 [14080/60032 (23%)]\tLoss: 0.308307\n",
            "Train Epoch: 1 [14720/60032 (25%)]\tLoss: 0.615366\n",
            "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.390446\n",
            "Train Epoch: 1 [16000/60032 (27%)]\tLoss: 0.305409\n",
            "Train Epoch: 1 [16640/60032 (28%)]\tLoss: 0.376919\n",
            "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.372103\n",
            "Train Epoch: 1 [17920/60032 (30%)]\tLoss: 0.406021\n",
            "Train Epoch: 1 [18560/60032 (31%)]\tLoss: 0.533358\n",
            "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.312826\n",
            "Train Epoch: 1 [19840/60032 (33%)]\tLoss: 0.273819\n",
            "Train Epoch: 1 [20480/60032 (34%)]\tLoss: 0.303732\n",
            "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 0.394617\n",
            "Train Epoch: 1 [21760/60032 (36%)]\tLoss: 0.269441\n",
            "Train Epoch: 1 [22400/60032 (37%)]\tLoss: 0.321690\n",
            "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.299298\n",
            "Train Epoch: 1 [23680/60032 (39%)]\tLoss: 0.262346\n",
            "Train Epoch: 1 [24320/60032 (41%)]\tLoss: 0.212261\n",
            "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.360453\n",
            "Train Epoch: 1 [25600/60032 (43%)]\tLoss: 0.434933\n",
            "Train Epoch: 1 [26240/60032 (44%)]\tLoss: 0.271046\n",
            "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.176902\n",
            "Train Epoch: 1 [27520/60032 (46%)]\tLoss: 0.257633\n",
            "Train Epoch: 1 [28160/60032 (47%)]\tLoss: 0.180958\n",
            "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.386504\n",
            "Train Epoch: 1 [29440/60032 (49%)]\tLoss: 0.332910\n",
            "Train Epoch: 1 [30080/60032 (50%)]\tLoss: 0.669243\n",
            "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.144631\n",
            "Train Epoch: 1 [31360/60032 (52%)]\tLoss: 0.232713\n",
            "Train Epoch: 1 [32000/60032 (53%)]\tLoss: 0.305170\n",
            "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.218618\n",
            "Train Epoch: 1 [33280/60032 (55%)]\tLoss: 0.225743\n",
            "Train Epoch: 1 [33920/60032 (57%)]\tLoss: 0.237649\n",
            "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.151107\n",
            "Train Epoch: 1 [35200/60032 (59%)]\tLoss: 0.290875\n",
            "Train Epoch: 1 [35840/60032 (60%)]\tLoss: 0.221027\n",
            "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.318646\n",
            "Train Epoch: 1 [37120/60032 (62%)]\tLoss: 0.274984\n",
            "Train Epoch: 1 [37760/60032 (63%)]\tLoss: 0.320128\n",
            "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.206872\n",
            "Train Epoch: 1 [39040/60032 (65%)]\tLoss: 0.194813\n",
            "Train Epoch: 1 [39680/60032 (66%)]\tLoss: 0.426123\n",
            "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 0.178330\n",
            "Train Epoch: 1 [40960/60032 (68%)]\tLoss: 0.210610\n",
            "Train Epoch: 1 [41600/60032 (69%)]\tLoss: 0.203684\n",
            "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.183487\n",
            "Train Epoch: 1 [42880/60032 (71%)]\tLoss: 0.109545\n",
            "Train Epoch: 1 [43520/60032 (72%)]\tLoss: 0.214975\n",
            "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.349123\n",
            "Train Epoch: 1 [44800/60032 (75%)]\tLoss: 0.228906\n",
            "Train Epoch: 1 [45440/60032 (76%)]\tLoss: 0.359266\n",
            "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.123895\n",
            "Train Epoch: 1 [46720/60032 (78%)]\tLoss: 0.212147\n",
            "Train Epoch: 1 [47360/60032 (79%)]\tLoss: 0.241777\n",
            "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.259264\n",
            "Train Epoch: 1 [48640/60032 (81%)]\tLoss: 0.263612\n",
            "Train Epoch: 1 [49280/60032 (82%)]\tLoss: 0.281947\n",
            "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.173935\n",
            "Train Epoch: 1 [50560/60032 (84%)]\tLoss: 0.149824\n",
            "Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.110589\n",
            "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.211629\n",
            "Train Epoch: 1 [52480/60032 (87%)]\tLoss: 0.304069\n",
            "Train Epoch: 1 [53120/60032 (88%)]\tLoss: 0.195024\n",
            "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.376578\n",
            "Train Epoch: 1 [54400/60032 (91%)]\tLoss: 0.175444\n",
            "Train Epoch: 1 [55040/60032 (92%)]\tLoss: 0.126718\n",
            "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.164699\n",
            "Train Epoch: 1 [56320/60032 (94%)]\tLoss: 0.150968\n",
            "Train Epoch: 1 [56960/60032 (95%)]\tLoss: 0.089778\n",
            "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.175772\n",
            "Train Epoch: 1 [58240/60032 (97%)]\tLoss: 0.251346\n",
            "Train Epoch: 1 [58880/60032 (98%)]\tLoss: 0.166730\n",
            "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.173113\n",
            "\n",
            "Test set: Average loss: 0.1714, Accuracy: 9488/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.136982\n",
            "Train Epoch: 2 [640/60032 (1%)]\tLoss: 0.131242\n",
            "Train Epoch: 2 [1280/60032 (2%)]\tLoss: 0.145176\n",
            "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.119726\n",
            "Train Epoch: 2 [2560/60032 (4%)]\tLoss: 0.200899\n",
            "Train Epoch: 2 [3200/60032 (5%)]\tLoss: 0.284986\n",
            "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.121851\n",
            "Train Epoch: 2 [4480/60032 (7%)]\tLoss: 0.144987\n",
            "Train Epoch: 2 [5120/60032 (9%)]\tLoss: 0.142260\n",
            "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.112734\n",
            "Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.189147\n",
            "Train Epoch: 2 [7040/60032 (12%)]\tLoss: 0.172275\n",
            "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.156510\n",
            "Train Epoch: 2 [8320/60032 (14%)]\tLoss: 0.071181\n",
            "Train Epoch: 2 [8960/60032 (15%)]\tLoss: 0.146095\n",
            "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.044013\n",
            "Train Epoch: 2 [10240/60032 (17%)]\tLoss: 0.172235\n",
            "Train Epoch: 2 [10880/60032 (18%)]\tLoss: 0.295077\n",
            "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.193669\n",
            "Train Epoch: 2 [12160/60032 (20%)]\tLoss: 0.150169\n",
            "Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.081198\n",
            "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.121817\n",
            "Train Epoch: 2 [14080/60032 (23%)]\tLoss: 0.277597\n",
            "Train Epoch: 2 [14720/60032 (25%)]\tLoss: 0.126618\n",
            "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.171404\n",
            "Train Epoch: 2 [16000/60032 (27%)]\tLoss: 0.196170\n",
            "Train Epoch: 2 [16640/60032 (28%)]\tLoss: 0.162388\n",
            "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.163096\n",
            "Train Epoch: 2 [17920/60032 (30%)]\tLoss: 0.095635\n",
            "Train Epoch: 2 [18560/60032 (31%)]\tLoss: 0.150551\n",
            "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.055939\n",
            "Train Epoch: 2 [19840/60032 (33%)]\tLoss: 0.087581\n",
            "Train Epoch: 2 [20480/60032 (34%)]\tLoss: 0.277243\n",
            "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.162521\n",
            "Train Epoch: 2 [21760/60032 (36%)]\tLoss: 0.268383\n",
            "Train Epoch: 2 [22400/60032 (37%)]\tLoss: 0.133994\n",
            "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.364789\n",
            "Train Epoch: 2 [23680/60032 (39%)]\tLoss: 0.083243\n",
            "Train Epoch: 2 [24320/60032 (41%)]\tLoss: 0.151107\n",
            "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.268702\n",
            "Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.031650\n",
            "Train Epoch: 2 [26240/60032 (44%)]\tLoss: 0.096981\n",
            "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.112181\n",
            "Train Epoch: 2 [27520/60032 (46%)]\tLoss: 0.103714\n",
            "Train Epoch: 2 [28160/60032 (47%)]\tLoss: 0.133328\n",
            "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.138665\n",
            "Train Epoch: 2 [29440/60032 (49%)]\tLoss: 0.041654\n",
            "Train Epoch: 2 [30080/60032 (50%)]\tLoss: 0.107013\n",
            "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.137944\n",
            "Train Epoch: 2 [31360/60032 (52%)]\tLoss: 0.078677\n",
            "Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.116954\n",
            "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.176856\n",
            "Train Epoch: 2 [33280/60032 (55%)]\tLoss: 0.030993\n",
            "Train Epoch: 2 [33920/60032 (57%)]\tLoss: 0.129508\n",
            "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.183357\n",
            "Train Epoch: 2 [35200/60032 (59%)]\tLoss: 0.111668\n",
            "Train Epoch: 2 [35840/60032 (60%)]\tLoss: 0.024881\n",
            "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.063785\n",
            "Train Epoch: 2 [37120/60032 (62%)]\tLoss: 0.075077\n",
            "Train Epoch: 2 [37760/60032 (63%)]\tLoss: 0.144636\n",
            "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.050517\n",
            "Train Epoch: 2 [39040/60032 (65%)]\tLoss: 0.121883\n",
            "Train Epoch: 2 [39680/60032 (66%)]\tLoss: 0.166800\n",
            "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 0.135950\n",
            "Train Epoch: 2 [40960/60032 (68%)]\tLoss: 0.065102\n",
            "Train Epoch: 2 [41600/60032 (69%)]\tLoss: 0.055656\n",
            "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.119884\n",
            "Train Epoch: 2 [42880/60032 (71%)]\tLoss: 0.129070\n",
            "Train Epoch: 2 [43520/60032 (72%)]\tLoss: 0.080352\n",
            "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.053117\n",
            "Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.129221\n",
            "Train Epoch: 2 [45440/60032 (76%)]\tLoss: 0.026495\n",
            "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.205546\n",
            "Train Epoch: 2 [46720/60032 (78%)]\tLoss: 0.293277\n",
            "Train Epoch: 2 [47360/60032 (79%)]\tLoss: 0.132079\n",
            "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.083449\n",
            "Train Epoch: 2 [48640/60032 (81%)]\tLoss: 0.093549\n",
            "Train Epoch: 2 [49280/60032 (82%)]\tLoss: 0.032769\n",
            "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.120211\n",
            "Train Epoch: 2 [50560/60032 (84%)]\tLoss: 0.091749\n",
            "Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.159344\n",
            "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.115711\n",
            "Train Epoch: 2 [52480/60032 (87%)]\tLoss: 0.036213\n",
            "Train Epoch: 2 [53120/60032 (88%)]\tLoss: 0.084314\n",
            "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.085498\n",
            "Train Epoch: 2 [54400/60032 (91%)]\tLoss: 0.051328\n",
            "Train Epoch: 2 [55040/60032 (92%)]\tLoss: 0.087684\n",
            "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.092397\n",
            "Train Epoch: 2 [56320/60032 (94%)]\tLoss: 0.116120\n",
            "Train Epoch: 2 [56960/60032 (95%)]\tLoss: 0.025299\n",
            "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.050432\n",
            "Train Epoch: 2 [58240/60032 (97%)]\tLoss: 0.073126\n",
            "Train Epoch: 2 [58880/60032 (98%)]\tLoss: 0.054885\n",
            "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.065184\n",
            "\n",
            "Test set: Average loss: 0.0881, Accuracy: 9739/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.196212\n",
            "Train Epoch: 3 [640/60032 (1%)]\tLoss: 0.027683\n",
            "Train Epoch: 3 [1280/60032 (2%)]\tLoss: 0.049790\n",
            "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.038210\n",
            "Train Epoch: 3 [2560/60032 (4%)]\tLoss: 0.062873\n",
            "Train Epoch: 3 [3200/60032 (5%)]\tLoss: 0.096338\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.038556\n",
            "Train Epoch: 3 [4480/60032 (7%)]\tLoss: 0.091497\n",
            "Train Epoch: 3 [5120/60032 (9%)]\tLoss: 0.186206\n",
            "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.097394\n",
            "Train Epoch: 3 [6400/60032 (11%)]\tLoss: 0.092199\n",
            "Train Epoch: 3 [7040/60032 (12%)]\tLoss: 0.158342\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.149393\n",
            "Train Epoch: 3 [8320/60032 (14%)]\tLoss: 0.108441\n",
            "Train Epoch: 3 [8960/60032 (15%)]\tLoss: 0.076324\n",
            "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.082731\n",
            "Train Epoch: 3 [10240/60032 (17%)]\tLoss: 0.074252\n",
            "Train Epoch: 3 [10880/60032 (18%)]\tLoss: 0.047882\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.121217\n",
            "Train Epoch: 3 [12160/60032 (20%)]\tLoss: 0.071879\n",
            "Train Epoch: 3 [12800/60032 (21%)]\tLoss: 0.086744\n",
            "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.036404\n",
            "Train Epoch: 3 [14080/60032 (23%)]\tLoss: 0.118092\n",
            "Train Epoch: 3 [14720/60032 (25%)]\tLoss: 0.128600\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.128535\n",
            "Train Epoch: 3 [16000/60032 (27%)]\tLoss: 0.122295\n",
            "Train Epoch: 3 [16640/60032 (28%)]\tLoss: 0.111799\n",
            "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.140395\n",
            "Train Epoch: 3 [17920/60032 (30%)]\tLoss: 0.051356\n",
            "Train Epoch: 3 [18560/60032 (31%)]\tLoss: 0.062547\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.082309\n",
            "Train Epoch: 3 [19840/60032 (33%)]\tLoss: 0.056778\n",
            "Train Epoch: 3 [20480/60032 (34%)]\tLoss: 0.045520\n",
            "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.051373\n",
            "Train Epoch: 3 [21760/60032 (36%)]\tLoss: 0.090650\n",
            "Train Epoch: 3 [22400/60032 (37%)]\tLoss: 0.036855\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.022794\n",
            "Train Epoch: 3 [23680/60032 (39%)]\tLoss: 0.137001\n",
            "Train Epoch: 3 [24320/60032 (41%)]\tLoss: 0.072350\n",
            "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.092482\n",
            "Train Epoch: 3 [25600/60032 (43%)]\tLoss: 0.021641\n",
            "Train Epoch: 3 [26240/60032 (44%)]\tLoss: 0.039240\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.077412\n",
            "Train Epoch: 3 [27520/60032 (46%)]\tLoss: 0.035526\n",
            "Train Epoch: 3 [28160/60032 (47%)]\tLoss: 0.049093\n",
            "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.019213\n",
            "Train Epoch: 3 [29440/60032 (49%)]\tLoss: 0.115915\n",
            "Train Epoch: 3 [30080/60032 (50%)]\tLoss: 0.022377\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.049881\n",
            "Train Epoch: 3 [31360/60032 (52%)]\tLoss: 0.215625\n",
            "Train Epoch: 3 [32000/60032 (53%)]\tLoss: 0.114067\n",
            "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.075174\n",
            "Train Epoch: 3 [33280/60032 (55%)]\tLoss: 0.025694\n",
            "Train Epoch: 3 [33920/60032 (57%)]\tLoss: 0.096331\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.037031\n",
            "Train Epoch: 3 [35200/60032 (59%)]\tLoss: 0.123958\n",
            "Train Epoch: 3 [35840/60032 (60%)]\tLoss: 0.111565\n",
            "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.040589\n",
            "Train Epoch: 3 [37120/60032 (62%)]\tLoss: 0.041095\n",
            "Train Epoch: 3 [37760/60032 (63%)]\tLoss: 0.081622\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.057721\n",
            "Train Epoch: 3 [39040/60032 (65%)]\tLoss: 0.046868\n",
            "Train Epoch: 3 [39680/60032 (66%)]\tLoss: 0.094902\n",
            "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.080666\n",
            "Train Epoch: 3 [40960/60032 (68%)]\tLoss: 0.117332\n",
            "Train Epoch: 3 [41600/60032 (69%)]\tLoss: 0.112522\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.107774\n",
            "Train Epoch: 3 [42880/60032 (71%)]\tLoss: 0.060894\n",
            "Train Epoch: 3 [43520/60032 (72%)]\tLoss: 0.058937\n",
            "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.142228\n",
            "Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.171978\n",
            "Train Epoch: 3 [45440/60032 (76%)]\tLoss: 0.093110\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.049810\n",
            "Train Epoch: 3 [46720/60032 (78%)]\tLoss: 0.055311\n",
            "Train Epoch: 3 [47360/60032 (79%)]\tLoss: 0.053296\n",
            "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.080236\n",
            "Train Epoch: 3 [48640/60032 (81%)]\tLoss: 0.033535\n",
            "Train Epoch: 3 [49280/60032 (82%)]\tLoss: 0.021829\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.057428\n",
            "Train Epoch: 3 [50560/60032 (84%)]\tLoss: 0.040245\n",
            "Train Epoch: 3 [51200/60032 (85%)]\tLoss: 0.143099\n",
            "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.215343\n",
            "Train Epoch: 3 [52480/60032 (87%)]\tLoss: 0.157364\n",
            "Train Epoch: 3 [53120/60032 (88%)]\tLoss: 0.048516\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.094795\n",
            "Train Epoch: 3 [54400/60032 (91%)]\tLoss: 0.064886\n",
            "Train Epoch: 3 [55040/60032 (92%)]\tLoss: 0.083711\n",
            "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.091166\n",
            "Train Epoch: 3 [56320/60032 (94%)]\tLoss: 0.133744\n",
            "Train Epoch: 3 [56960/60032 (95%)]\tLoss: 0.160619\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.027220\n",
            "Train Epoch: 3 [58240/60032 (97%)]\tLoss: 0.017269\n",
            "Train Epoch: 3 [58880/60032 (98%)]\tLoss: 0.061630\n",
            "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.037820\n",
            "\n",
            "Test set: Average loss: 0.0815, Accuracy: 9750/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.034388\n",
            "Train Epoch: 4 [640/60032 (1%)]\tLoss: 0.008091\n",
            "Train Epoch: 4 [1280/60032 (2%)]\tLoss: 0.039770\n",
            "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.132797\n",
            "Train Epoch: 4 [2560/60032 (4%)]\tLoss: 0.046503\n",
            "Train Epoch: 4 [3200/60032 (5%)]\tLoss: 0.079162\n",
            "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.052506\n",
            "Train Epoch: 4 [4480/60032 (7%)]\tLoss: 0.112856\n",
            "Train Epoch: 4 [5120/60032 (9%)]\tLoss: 0.047054\n",
            "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.077705\n",
            "Train Epoch: 4 [6400/60032 (11%)]\tLoss: 0.035396\n",
            "Train Epoch: 4 [7040/60032 (12%)]\tLoss: 0.058383\n",
            "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.032164\n",
            "Train Epoch: 4 [8320/60032 (14%)]\tLoss: 0.058356\n",
            "Train Epoch: 4 [8960/60032 (15%)]\tLoss: 0.091825\n",
            "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.088521\n",
            "Train Epoch: 4 [10240/60032 (17%)]\tLoss: 0.019547\n",
            "Train Epoch: 4 [10880/60032 (18%)]\tLoss: 0.058892\n",
            "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.059722\n",
            "Train Epoch: 4 [12160/60032 (20%)]\tLoss: 0.175013\n",
            "Train Epoch: 4 [12800/60032 (21%)]\tLoss: 0.085420\n",
            "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.085398\n",
            "Train Epoch: 4 [14080/60032 (23%)]\tLoss: 0.066407\n",
            "Train Epoch: 4 [14720/60032 (25%)]\tLoss: 0.086988\n",
            "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.094427\n",
            "Train Epoch: 4 [16000/60032 (27%)]\tLoss: 0.030238\n",
            "Train Epoch: 4 [16640/60032 (28%)]\tLoss: 0.045739\n",
            "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.022443\n",
            "Train Epoch: 4 [17920/60032 (30%)]\tLoss: 0.022746\n",
            "Train Epoch: 4 [18560/60032 (31%)]\tLoss: 0.073060\n",
            "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.065248\n",
            "Train Epoch: 4 [19840/60032 (33%)]\tLoss: 0.027066\n",
            "Train Epoch: 4 [20480/60032 (34%)]\tLoss: 0.014112\n",
            "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.031931\n",
            "Train Epoch: 4 [21760/60032 (36%)]\tLoss: 0.047272\n",
            "Train Epoch: 4 [22400/60032 (37%)]\tLoss: 0.116918\n",
            "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.159644\n",
            "Train Epoch: 4 [23680/60032 (39%)]\tLoss: 0.059512\n",
            "Train Epoch: 4 [24320/60032 (41%)]\tLoss: 0.077351\n",
            "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.238766\n",
            "Train Epoch: 4 [25600/60032 (43%)]\tLoss: 0.098239\n",
            "Train Epoch: 4 [26240/60032 (44%)]\tLoss: 0.041562\n",
            "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.023171\n",
            "Train Epoch: 4 [27520/60032 (46%)]\tLoss: 0.042277\n",
            "Train Epoch: 4 [28160/60032 (47%)]\tLoss: 0.037537\n",
            "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.054304\n",
            "Train Epoch: 4 [29440/60032 (49%)]\tLoss: 0.037641\n",
            "Train Epoch: 4 [30080/60032 (50%)]\tLoss: 0.045417\n",
            "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.032511\n",
            "Train Epoch: 4 [31360/60032 (52%)]\tLoss: 0.081783\n",
            "Train Epoch: 4 [32000/60032 (53%)]\tLoss: 0.117212\n",
            "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.044074\n",
            "Train Epoch: 4 [33280/60032 (55%)]\tLoss: 0.084898\n",
            "Train Epoch: 4 [33920/60032 (57%)]\tLoss: 0.128583\n",
            "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.029901\n",
            "Train Epoch: 4 [35200/60032 (59%)]\tLoss: 0.035927\n",
            "Train Epoch: 4 [35840/60032 (60%)]\tLoss: 0.097918\n",
            "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.025856\n",
            "Train Epoch: 4 [37120/60032 (62%)]\tLoss: 0.070445\n",
            "Train Epoch: 4 [37760/60032 (63%)]\tLoss: 0.018995\n",
            "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.077177\n",
            "Train Epoch: 4 [39040/60032 (65%)]\tLoss: 0.044427\n",
            "Train Epoch: 4 [39680/60032 (66%)]\tLoss: 0.110785\n",
            "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.118569\n",
            "Train Epoch: 4 [40960/60032 (68%)]\tLoss: 0.148673\n",
            "Train Epoch: 4 [41600/60032 (69%)]\tLoss: 0.052738\n",
            "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.016911\n",
            "Train Epoch: 4 [42880/60032 (71%)]\tLoss: 0.045542\n",
            "Train Epoch: 4 [43520/60032 (72%)]\tLoss: 0.044842\n",
            "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.045369\n",
            "Train Epoch: 4 [44800/60032 (75%)]\tLoss: 0.010688\n",
            "Train Epoch: 4 [45440/60032 (76%)]\tLoss: 0.015408\n",
            "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.017765\n",
            "Train Epoch: 4 [46720/60032 (78%)]\tLoss: 0.074281\n",
            "Train Epoch: 4 [47360/60032 (79%)]\tLoss: 0.081282\n",
            "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.026275\n",
            "Train Epoch: 4 [48640/60032 (81%)]\tLoss: 0.019319\n",
            "Train Epoch: 4 [49280/60032 (82%)]\tLoss: 0.166881\n",
            "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.093638\n",
            "Train Epoch: 4 [50560/60032 (84%)]\tLoss: 0.044577\n",
            "Train Epoch: 4 [51200/60032 (85%)]\tLoss: 0.072052\n",
            "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.040266\n",
            "Train Epoch: 4 [52480/60032 (87%)]\tLoss: 0.156111\n",
            "Train Epoch: 4 [53120/60032 (88%)]\tLoss: 0.089448\n",
            "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.142163\n",
            "Train Epoch: 4 [54400/60032 (91%)]\tLoss: 0.045538\n",
            "Train Epoch: 4 [55040/60032 (92%)]\tLoss: 0.068240\n",
            "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.051040\n",
            "Train Epoch: 4 [56320/60032 (94%)]\tLoss: 0.113153\n",
            "Train Epoch: 4 [56960/60032 (95%)]\tLoss: 0.093316\n",
            "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.063761\n",
            "Train Epoch: 4 [58240/60032 (97%)]\tLoss: 0.107417\n",
            "Train Epoch: 4 [58880/60032 (98%)]\tLoss: 0.045168\n",
            "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.016856\n",
            "\n",
            "Test set: Average loss: 0.0568, Accuracy: 9814/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.020153\n",
            "Train Epoch: 5 [640/60032 (1%)]\tLoss: 0.027963\n",
            "Train Epoch: 5 [1280/60032 (2%)]\tLoss: 0.177104\n",
            "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.026191\n",
            "Train Epoch: 5 [2560/60032 (4%)]\tLoss: 0.023304\n",
            "Train Epoch: 5 [3200/60032 (5%)]\tLoss: 0.018702\n",
            "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.026195\n",
            "Train Epoch: 5 [4480/60032 (7%)]\tLoss: 0.046623\n",
            "Train Epoch: 5 [5120/60032 (9%)]\tLoss: 0.114872\n",
            "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.108763\n",
            "Train Epoch: 5 [6400/60032 (11%)]\tLoss: 0.016490\n",
            "Train Epoch: 5 [7040/60032 (12%)]\tLoss: 0.048616\n",
            "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.010754\n",
            "Train Epoch: 5 [8320/60032 (14%)]\tLoss: 0.106226\n",
            "Train Epoch: 5 [8960/60032 (15%)]\tLoss: 0.087185\n",
            "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.085100\n",
            "Train Epoch: 5 [10240/60032 (17%)]\tLoss: 0.112376\n",
            "Train Epoch: 5 [10880/60032 (18%)]\tLoss: 0.087230\n",
            "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.060482\n",
            "Train Epoch: 5 [12160/60032 (20%)]\tLoss: 0.027084\n",
            "Train Epoch: 5 [12800/60032 (21%)]\tLoss: 0.022883\n",
            "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.031407\n",
            "Train Epoch: 5 [14080/60032 (23%)]\tLoss: 0.220639\n",
            "Train Epoch: 5 [14720/60032 (25%)]\tLoss: 0.049089\n",
            "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.084462\n",
            "Train Epoch: 5 [16000/60032 (27%)]\tLoss: 0.015038\n",
            "Train Epoch: 5 [16640/60032 (28%)]\tLoss: 0.037550\n",
            "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.028653\n",
            "Train Epoch: 5 [17920/60032 (30%)]\tLoss: 0.028285\n",
            "Train Epoch: 5 [18560/60032 (31%)]\tLoss: 0.150356\n",
            "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.036568\n",
            "Train Epoch: 5 [19840/60032 (33%)]\tLoss: 0.018198\n",
            "Train Epoch: 5 [20480/60032 (34%)]\tLoss: 0.218620\n",
            "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.031804\n",
            "Train Epoch: 5 [21760/60032 (36%)]\tLoss: 0.025349\n",
            "Train Epoch: 5 [22400/60032 (37%)]\tLoss: 0.090573\n",
            "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.065329\n",
            "Train Epoch: 5 [23680/60032 (39%)]\tLoss: 0.032602\n",
            "Train Epoch: 5 [24320/60032 (41%)]\tLoss: 0.059301\n",
            "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.082533\n",
            "Train Epoch: 5 [25600/60032 (43%)]\tLoss: 0.177886\n",
            "Train Epoch: 5 [26240/60032 (44%)]\tLoss: 0.041511\n",
            "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.051170\n",
            "Train Epoch: 5 [27520/60032 (46%)]\tLoss: 0.059319\n",
            "Train Epoch: 5 [28160/60032 (47%)]\tLoss: 0.175344\n",
            "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.034557\n",
            "Train Epoch: 5 [29440/60032 (49%)]\tLoss: 0.028588\n",
            "Train Epoch: 5 [30080/60032 (50%)]\tLoss: 0.119612\n",
            "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.090046\n",
            "Train Epoch: 5 [31360/60032 (52%)]\tLoss: 0.046450\n",
            "Train Epoch: 5 [32000/60032 (53%)]\tLoss: 0.073137\n",
            "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.083834\n",
            "Train Epoch: 5 [33280/60032 (55%)]\tLoss: 0.038151\n",
            "Train Epoch: 5 [33920/60032 (57%)]\tLoss: 0.110557\n",
            "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.045256\n",
            "Train Epoch: 5 [35200/60032 (59%)]\tLoss: 0.194613\n",
            "Train Epoch: 5 [35840/60032 (60%)]\tLoss: 0.009550\n",
            "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.057306\n",
            "Train Epoch: 5 [37120/60032 (62%)]\tLoss: 0.026960\n",
            "Train Epoch: 5 [37760/60032 (63%)]\tLoss: 0.116204\n",
            "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.033455\n",
            "Train Epoch: 5 [39040/60032 (65%)]\tLoss: 0.024697\n",
            "Train Epoch: 5 [39680/60032 (66%)]\tLoss: 0.090211\n",
            "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.071277\n",
            "Train Epoch: 5 [40960/60032 (68%)]\tLoss: 0.009953\n",
            "Train Epoch: 5 [41600/60032 (69%)]\tLoss: 0.039897\n",
            "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.052740\n",
            "Train Epoch: 5 [42880/60032 (71%)]\tLoss: 0.133722\n",
            "Train Epoch: 5 [43520/60032 (72%)]\tLoss: 0.045977\n",
            "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.051510\n",
            "Train Epoch: 5 [44800/60032 (75%)]\tLoss: 0.045732\n",
            "Train Epoch: 5 [45440/60032 (76%)]\tLoss: 0.016773\n",
            "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.062058\n",
            "Train Epoch: 5 [46720/60032 (78%)]\tLoss: 0.034382\n",
            "Train Epoch: 5 [47360/60032 (79%)]\tLoss: 0.035736\n",
            "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.115613\n",
            "Train Epoch: 5 [48640/60032 (81%)]\tLoss: 0.112691\n",
            "Train Epoch: 5 [49280/60032 (82%)]\tLoss: 0.014247\n",
            "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.024166\n",
            "Train Epoch: 5 [50560/60032 (84%)]\tLoss: 0.185810\n",
            "Train Epoch: 5 [51200/60032 (85%)]\tLoss: 0.059340\n",
            "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.058731\n",
            "Train Epoch: 5 [52480/60032 (87%)]\tLoss: 0.055707\n",
            "Train Epoch: 5 [53120/60032 (88%)]\tLoss: 0.068775\n",
            "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.018685\n",
            "Train Epoch: 5 [54400/60032 (91%)]\tLoss: 0.013157\n",
            "Train Epoch: 5 [55040/60032 (92%)]\tLoss: 0.030340\n",
            "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.054379\n",
            "Train Epoch: 5 [56320/60032 (94%)]\tLoss: 0.108384\n",
            "Train Epoch: 5 [56960/60032 (95%)]\tLoss: 0.052427\n",
            "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.020344\n",
            "Train Epoch: 5 [58240/60032 (97%)]\tLoss: 0.009642\n",
            "Train Epoch: 5 [58880/60032 (98%)]\tLoss: 0.028303\n",
            "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.087671\n",
            "\n",
            "Test set: Average loss: 0.0533, Accuracy: 9836/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.025014\n",
            "Train Epoch: 6 [640/60032 (1%)]\tLoss: 0.028698\n",
            "Train Epoch: 6 [1280/60032 (2%)]\tLoss: 0.064421\n",
            "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.043281\n",
            "Train Epoch: 6 [2560/60032 (4%)]\tLoss: 0.016751\n",
            "Train Epoch: 6 [3200/60032 (5%)]\tLoss: 0.102817\n",
            "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.025380\n",
            "Train Epoch: 6 [4480/60032 (7%)]\tLoss: 0.062252\n",
            "Train Epoch: 6 [5120/60032 (9%)]\tLoss: 0.029176\n",
            "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.049006\n",
            "Train Epoch: 6 [6400/60032 (11%)]\tLoss: 0.020854\n",
            "Train Epoch: 6 [7040/60032 (12%)]\tLoss: 0.053753\n",
            "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.073204\n",
            "Train Epoch: 6 [8320/60032 (14%)]\tLoss: 0.092593\n",
            "Train Epoch: 6 [8960/60032 (15%)]\tLoss: 0.045797\n",
            "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.067838\n",
            "Train Epoch: 6 [10240/60032 (17%)]\tLoss: 0.006947\n",
            "Train Epoch: 6 [10880/60032 (18%)]\tLoss: 0.012980\n",
            "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.029988\n",
            "Train Epoch: 6 [12160/60032 (20%)]\tLoss: 0.016550\n",
            "Train Epoch: 6 [12800/60032 (21%)]\tLoss: 0.030582\n",
            "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.069579\n",
            "Train Epoch: 6 [14080/60032 (23%)]\tLoss: 0.029539\n",
            "Train Epoch: 6 [14720/60032 (25%)]\tLoss: 0.044889\n",
            "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.178108\n",
            "Train Epoch: 6 [16000/60032 (27%)]\tLoss: 0.027673\n",
            "Train Epoch: 6 [16640/60032 (28%)]\tLoss: 0.047966\n",
            "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.038432\n",
            "Train Epoch: 6 [17920/60032 (30%)]\tLoss: 0.102820\n",
            "Train Epoch: 6 [18560/60032 (31%)]\tLoss: 0.150295\n",
            "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.044012\n",
            "Train Epoch: 6 [19840/60032 (33%)]\tLoss: 0.011486\n",
            "Train Epoch: 6 [20480/60032 (34%)]\tLoss: 0.006812\n",
            "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.016451\n",
            "Train Epoch: 6 [21760/60032 (36%)]\tLoss: 0.014458\n",
            "Train Epoch: 6 [22400/60032 (37%)]\tLoss: 0.053117\n",
            "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.040178\n",
            "Train Epoch: 6 [23680/60032 (39%)]\tLoss: 0.023645\n",
            "Train Epoch: 6 [24320/60032 (41%)]\tLoss: 0.043930\n",
            "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.019009\n",
            "Train Epoch: 6 [25600/60032 (43%)]\tLoss: 0.052494\n",
            "Train Epoch: 6 [26240/60032 (44%)]\tLoss: 0.010974\n",
            "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.141218\n",
            "Train Epoch: 6 [27520/60032 (46%)]\tLoss: 0.042667\n",
            "Train Epoch: 6 [28160/60032 (47%)]\tLoss: 0.081838\n",
            "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.019254\n",
            "Train Epoch: 6 [29440/60032 (49%)]\tLoss: 0.050912\n",
            "Train Epoch: 6 [30080/60032 (50%)]\tLoss: 0.020576\n",
            "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.043702\n",
            "Train Epoch: 6 [31360/60032 (52%)]\tLoss: 0.037019\n",
            "Train Epoch: 6 [32000/60032 (53%)]\tLoss: 0.054425\n",
            "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.061798\n",
            "Train Epoch: 6 [33280/60032 (55%)]\tLoss: 0.036123\n",
            "Train Epoch: 6 [33920/60032 (57%)]\tLoss: 0.044808\n",
            "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.053852\n",
            "Train Epoch: 6 [35200/60032 (59%)]\tLoss: 0.005029\n",
            "Train Epoch: 6 [35840/60032 (60%)]\tLoss: 0.007349\n",
            "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.091853\n",
            "Train Epoch: 6 [37120/60032 (62%)]\tLoss: 0.016010\n",
            "Train Epoch: 6 [37760/60032 (63%)]\tLoss: 0.069505\n",
            "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.021467\n",
            "Train Epoch: 6 [39040/60032 (65%)]\tLoss: 0.027197\n",
            "Train Epoch: 6 [39680/60032 (66%)]\tLoss: 0.073208\n",
            "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.007481\n",
            "Train Epoch: 6 [40960/60032 (68%)]\tLoss: 0.176389\n",
            "Train Epoch: 6 [41600/60032 (69%)]\tLoss: 0.057661\n",
            "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.031201\n",
            "Train Epoch: 6 [42880/60032 (71%)]\tLoss: 0.020894\n",
            "Train Epoch: 6 [43520/60032 (72%)]\tLoss: 0.090538\n",
            "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.029354\n",
            "Train Epoch: 6 [44800/60032 (75%)]\tLoss: 0.021749\n",
            "Train Epoch: 6 [45440/60032 (76%)]\tLoss: 0.008537\n",
            "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.060737\n",
            "Train Epoch: 6 [46720/60032 (78%)]\tLoss: 0.055961\n",
            "Train Epoch: 6 [47360/60032 (79%)]\tLoss: 0.207721\n",
            "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.016195\n",
            "Train Epoch: 6 [48640/60032 (81%)]\tLoss: 0.046007\n",
            "Train Epoch: 6 [49280/60032 (82%)]\tLoss: 0.008330\n",
            "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.018175\n",
            "Train Epoch: 6 [50560/60032 (84%)]\tLoss: 0.026913\n",
            "Train Epoch: 6 [51200/60032 (85%)]\tLoss: 0.062922\n",
            "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.114062\n",
            "Train Epoch: 6 [52480/60032 (87%)]\tLoss: 0.033819\n",
            "Train Epoch: 6 [53120/60032 (88%)]\tLoss: 0.116976\n",
            "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.041320\n",
            "Train Epoch: 6 [54400/60032 (91%)]\tLoss: 0.010514\n",
            "Train Epoch: 6 [55040/60032 (92%)]\tLoss: 0.019040\n",
            "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.004178\n",
            "Train Epoch: 6 [56320/60032 (94%)]\tLoss: 0.023471\n",
            "Train Epoch: 6 [56960/60032 (95%)]\tLoss: 0.022724\n",
            "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.058093\n",
            "Train Epoch: 6 [58240/60032 (97%)]\tLoss: 0.039848\n",
            "Train Epoch: 6 [58880/60032 (98%)]\tLoss: 0.043645\n",
            "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.075563\n",
            "\n",
            "Test set: Average loss: 0.0475, Accuracy: 9853/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.146034\n",
            "Train Epoch: 7 [640/60032 (1%)]\tLoss: 0.008706\n",
            "Train Epoch: 7 [1280/60032 (2%)]\tLoss: 0.130339\n",
            "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.022218\n",
            "Train Epoch: 7 [2560/60032 (4%)]\tLoss: 0.004462\n",
            "Train Epoch: 7 [3200/60032 (5%)]\tLoss: 0.023724\n",
            "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.008415\n",
            "Train Epoch: 7 [4480/60032 (7%)]\tLoss: 0.056233\n",
            "Train Epoch: 7 [5120/60032 (9%)]\tLoss: 0.164741\n",
            "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.037281\n",
            "Train Epoch: 7 [6400/60032 (11%)]\tLoss: 0.036307\n",
            "Train Epoch: 7 [7040/60032 (12%)]\tLoss: 0.017701\n",
            "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.008410\n",
            "Train Epoch: 7 [8320/60032 (14%)]\tLoss: 0.022548\n",
            "Train Epoch: 7 [8960/60032 (15%)]\tLoss: 0.026380\n",
            "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.024258\n",
            "Train Epoch: 7 [10240/60032 (17%)]\tLoss: 0.011242\n",
            "Train Epoch: 7 [10880/60032 (18%)]\tLoss: 0.041713\n",
            "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.016677\n",
            "Train Epoch: 7 [12160/60032 (20%)]\tLoss: 0.024791\n",
            "Train Epoch: 7 [12800/60032 (21%)]\tLoss: 0.071365\n",
            "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.008650\n",
            "Train Epoch: 7 [14080/60032 (23%)]\tLoss: 0.129404\n",
            "Train Epoch: 7 [14720/60032 (25%)]\tLoss: 0.018914\n",
            "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.055319\n",
            "Train Epoch: 7 [16000/60032 (27%)]\tLoss: 0.039595\n",
            "Train Epoch: 7 [16640/60032 (28%)]\tLoss: 0.015238\n",
            "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.011442\n",
            "Train Epoch: 7 [17920/60032 (30%)]\tLoss: 0.026230\n",
            "Train Epoch: 7 [18560/60032 (31%)]\tLoss: 0.007713\n",
            "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.074849\n",
            "Train Epoch: 7 [19840/60032 (33%)]\tLoss: 0.109825\n",
            "Train Epoch: 7 [20480/60032 (34%)]\tLoss: 0.075948\n",
            "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.052406\n",
            "Train Epoch: 7 [21760/60032 (36%)]\tLoss: 0.006732\n",
            "Train Epoch: 7 [22400/60032 (37%)]\tLoss: 0.036843\n",
            "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.026656\n",
            "Train Epoch: 7 [23680/60032 (39%)]\tLoss: 0.096858\n",
            "Train Epoch: 7 [24320/60032 (41%)]\tLoss: 0.044328\n",
            "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.022555\n",
            "Train Epoch: 7 [25600/60032 (43%)]\tLoss: 0.079736\n",
            "Train Epoch: 7 [26240/60032 (44%)]\tLoss: 0.008342\n",
            "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.012412\n",
            "Train Epoch: 7 [27520/60032 (46%)]\tLoss: 0.022173\n",
            "Train Epoch: 7 [28160/60032 (47%)]\tLoss: 0.028194\n",
            "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.086265\n",
            "Train Epoch: 7 [29440/60032 (49%)]\tLoss: 0.051854\n",
            "Train Epoch: 7 [30080/60032 (50%)]\tLoss: 0.044413\n",
            "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.011174\n",
            "Train Epoch: 7 [31360/60032 (52%)]\tLoss: 0.010169\n",
            "Train Epoch: 7 [32000/60032 (53%)]\tLoss: 0.004514\n",
            "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.024473\n",
            "Train Epoch: 7 [33280/60032 (55%)]\tLoss: 0.005404\n",
            "Train Epoch: 7 [33920/60032 (57%)]\tLoss: 0.021634\n",
            "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.018731\n",
            "Train Epoch: 7 [35200/60032 (59%)]\tLoss: 0.109460\n",
            "Train Epoch: 7 [35840/60032 (60%)]\tLoss: 0.074583\n",
            "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.082778\n",
            "Train Epoch: 7 [37120/60032 (62%)]\tLoss: 0.029774\n",
            "Train Epoch: 7 [37760/60032 (63%)]\tLoss: 0.143449\n",
            "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.019281\n",
            "Train Epoch: 7 [39040/60032 (65%)]\tLoss: 0.033991\n",
            "Train Epoch: 7 [39680/60032 (66%)]\tLoss: 0.014584\n",
            "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 0.038756\n",
            "Train Epoch: 7 [40960/60032 (68%)]\tLoss: 0.034623\n",
            "Train Epoch: 7 [41600/60032 (69%)]\tLoss: 0.043312\n",
            "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.105817\n",
            "Train Epoch: 7 [42880/60032 (71%)]\tLoss: 0.020727\n",
            "Train Epoch: 7 [43520/60032 (72%)]\tLoss: 0.120223\n",
            "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.027469\n",
            "Train Epoch: 7 [44800/60032 (75%)]\tLoss: 0.009150\n",
            "Train Epoch: 7 [45440/60032 (76%)]\tLoss: 0.008214\n",
            "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.083218\n",
            "Train Epoch: 7 [46720/60032 (78%)]\tLoss: 0.055152\n",
            "Train Epoch: 7 [47360/60032 (79%)]\tLoss: 0.038762\n",
            "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.007858\n",
            "Train Epoch: 7 [48640/60032 (81%)]\tLoss: 0.004001\n",
            "Train Epoch: 7 [49280/60032 (82%)]\tLoss: 0.070956\n",
            "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.051868\n",
            "Train Epoch: 7 [50560/60032 (84%)]\tLoss: 0.065362\n",
            "Train Epoch: 7 [51200/60032 (85%)]\tLoss: 0.059228\n",
            "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.039857\n",
            "Train Epoch: 7 [52480/60032 (87%)]\tLoss: 0.112386\n",
            "Train Epoch: 7 [53120/60032 (88%)]\tLoss: 0.050384\n",
            "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.043221\n",
            "Train Epoch: 7 [54400/60032 (91%)]\tLoss: 0.009287\n",
            "Train Epoch: 7 [55040/60032 (92%)]\tLoss: 0.025575\n",
            "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.009971\n",
            "Train Epoch: 7 [56320/60032 (94%)]\tLoss: 0.004897\n",
            "Train Epoch: 7 [56960/60032 (95%)]\tLoss: 0.113037\n",
            "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.116349\n",
            "Train Epoch: 7 [58240/60032 (97%)]\tLoss: 0.194649\n",
            "Train Epoch: 7 [58880/60032 (98%)]\tLoss: 0.020764\n",
            "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.016565\n",
            "\n",
            "Test set: Average loss: 0.0439, Accuracy: 9858/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.033885\n",
            "Train Epoch: 8 [640/60032 (1%)]\tLoss: 0.191931\n",
            "Train Epoch: 8 [1280/60032 (2%)]\tLoss: 0.042910\n",
            "Train Epoch: 8 [1920/60032 (3%)]\tLoss: 0.054583\n",
            "Train Epoch: 8 [2560/60032 (4%)]\tLoss: 0.041497\n",
            "Train Epoch: 8 [3200/60032 (5%)]\tLoss: 0.009083\n",
            "Train Epoch: 8 [3840/60032 (6%)]\tLoss: 0.010484\n",
            "Train Epoch: 8 [4480/60032 (7%)]\tLoss: 0.049081\n",
            "Train Epoch: 8 [5120/60032 (9%)]\tLoss: 0.014216\n",
            "Train Epoch: 8 [5760/60032 (10%)]\tLoss: 0.003393\n",
            "Train Epoch: 8 [6400/60032 (11%)]\tLoss: 0.015030\n",
            "Train Epoch: 8 [7040/60032 (12%)]\tLoss: 0.076544\n",
            "Train Epoch: 8 [7680/60032 (13%)]\tLoss: 0.018571\n",
            "Train Epoch: 8 [8320/60032 (14%)]\tLoss: 0.044120\n",
            "Train Epoch: 8 [8960/60032 (15%)]\tLoss: 0.017073\n",
            "Train Epoch: 8 [9600/60032 (16%)]\tLoss: 0.017347\n",
            "Train Epoch: 8 [10240/60032 (17%)]\tLoss: 0.005205\n",
            "Train Epoch: 8 [10880/60032 (18%)]\tLoss: 0.048844\n",
            "Train Epoch: 8 [11520/60032 (19%)]\tLoss: 0.015564\n",
            "Train Epoch: 8 [12160/60032 (20%)]\tLoss: 0.024815\n",
            "Train Epoch: 8 [12800/60032 (21%)]\tLoss: 0.071916\n",
            "Train Epoch: 8 [13440/60032 (22%)]\tLoss: 0.041922\n",
            "Train Epoch: 8 [14080/60032 (23%)]\tLoss: 0.012463\n",
            "Train Epoch: 8 [14720/60032 (25%)]\tLoss: 0.041193\n",
            "Train Epoch: 8 [15360/60032 (26%)]\tLoss: 0.004859\n",
            "Train Epoch: 8 [16000/60032 (27%)]\tLoss: 0.080889\n",
            "Train Epoch: 8 [16640/60032 (28%)]\tLoss: 0.058989\n",
            "Train Epoch: 8 [17280/60032 (29%)]\tLoss: 0.072190\n",
            "Train Epoch: 8 [17920/60032 (30%)]\tLoss: 0.084213\n",
            "Train Epoch: 8 [18560/60032 (31%)]\tLoss: 0.040844\n",
            "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.045398\n",
            "Train Epoch: 8 [19840/60032 (33%)]\tLoss: 0.031646\n",
            "Train Epoch: 8 [20480/60032 (34%)]\tLoss: 0.065922\n",
            "Train Epoch: 8 [21120/60032 (35%)]\tLoss: 0.028012\n",
            "Train Epoch: 8 [21760/60032 (36%)]\tLoss: 0.051286\n",
            "Train Epoch: 8 [22400/60032 (37%)]\tLoss: 0.130897\n",
            "Train Epoch: 8 [23040/60032 (38%)]\tLoss: 0.009301\n",
            "Train Epoch: 8 [23680/60032 (39%)]\tLoss: 0.025238\n",
            "Train Epoch: 8 [24320/60032 (41%)]\tLoss: 0.065967\n",
            "Train Epoch: 8 [24960/60032 (42%)]\tLoss: 0.008086\n",
            "Train Epoch: 8 [25600/60032 (43%)]\tLoss: 0.030019\n",
            "Train Epoch: 8 [26240/60032 (44%)]\tLoss: 0.113557\n",
            "Train Epoch: 8 [26880/60032 (45%)]\tLoss: 0.008590\n",
            "Train Epoch: 8 [27520/60032 (46%)]\tLoss: 0.027831\n",
            "Train Epoch: 8 [28160/60032 (47%)]\tLoss: 0.053981\n",
            "Train Epoch: 8 [28800/60032 (48%)]\tLoss: 0.090621\n",
            "Train Epoch: 8 [29440/60032 (49%)]\tLoss: 0.016549\n",
            "Train Epoch: 8 [30080/60032 (50%)]\tLoss: 0.040018\n",
            "Train Epoch: 8 [30720/60032 (51%)]\tLoss: 0.004209\n",
            "Train Epoch: 8 [31360/60032 (52%)]\tLoss: 0.004295\n",
            "Train Epoch: 8 [32000/60032 (53%)]\tLoss: 0.047793\n",
            "Train Epoch: 8 [32640/60032 (54%)]\tLoss: 0.051148\n",
            "Train Epoch: 8 [33280/60032 (55%)]\tLoss: 0.034580\n",
            "Train Epoch: 8 [33920/60032 (57%)]\tLoss: 0.062458\n",
            "Train Epoch: 8 [34560/60032 (58%)]\tLoss: 0.012093\n",
            "Train Epoch: 8 [35200/60032 (59%)]\tLoss: 0.136102\n",
            "Train Epoch: 8 [35840/60032 (60%)]\tLoss: 0.003715\n",
            "Train Epoch: 8 [36480/60032 (61%)]\tLoss: 0.086116\n",
            "Train Epoch: 8 [37120/60032 (62%)]\tLoss: 0.006770\n",
            "Train Epoch: 8 [37760/60032 (63%)]\tLoss: 0.006999\n",
            "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.127277\n",
            "Train Epoch: 8 [39040/60032 (65%)]\tLoss: 0.214078\n",
            "Train Epoch: 8 [39680/60032 (66%)]\tLoss: 0.040323\n",
            "Train Epoch: 8 [40320/60032 (67%)]\tLoss: 0.041333\n",
            "Train Epoch: 8 [40960/60032 (68%)]\tLoss: 0.026706\n",
            "Train Epoch: 8 [41600/60032 (69%)]\tLoss: 0.021022\n",
            "Train Epoch: 8 [42240/60032 (70%)]\tLoss: 0.042266\n",
            "Train Epoch: 8 [42880/60032 (71%)]\tLoss: 0.030710\n",
            "Train Epoch: 8 [43520/60032 (72%)]\tLoss: 0.007389\n",
            "Train Epoch: 8 [44160/60032 (74%)]\tLoss: 0.017878\n",
            "Train Epoch: 8 [44800/60032 (75%)]\tLoss: 0.041308\n",
            "Train Epoch: 8 [45440/60032 (76%)]\tLoss: 0.051370\n",
            "Train Epoch: 8 [46080/60032 (77%)]\tLoss: 0.075144\n",
            "Train Epoch: 8 [46720/60032 (78%)]\tLoss: 0.004884\n",
            "Train Epoch: 8 [47360/60032 (79%)]\tLoss: 0.012424\n",
            "Train Epoch: 8 [48000/60032 (80%)]\tLoss: 0.023663\n",
            "Train Epoch: 8 [48640/60032 (81%)]\tLoss: 0.161230\n",
            "Train Epoch: 8 [49280/60032 (82%)]\tLoss: 0.027597\n",
            "Train Epoch: 8 [49920/60032 (83%)]\tLoss: 0.039839\n",
            "Train Epoch: 8 [50560/60032 (84%)]\tLoss: 0.043766\n",
            "Train Epoch: 8 [51200/60032 (85%)]\tLoss: 0.038147\n",
            "Train Epoch: 8 [51840/60032 (86%)]\tLoss: 0.018665\n",
            "Train Epoch: 8 [52480/60032 (87%)]\tLoss: 0.006185\n",
            "Train Epoch: 8 [53120/60032 (88%)]\tLoss: 0.004579\n",
            "Train Epoch: 8 [53760/60032 (90%)]\tLoss: 0.007689\n",
            "Train Epoch: 8 [54400/60032 (91%)]\tLoss: 0.001980\n",
            "Train Epoch: 8 [55040/60032 (92%)]\tLoss: 0.016289\n",
            "Train Epoch: 8 [55680/60032 (93%)]\tLoss: 0.056811\n",
            "Train Epoch: 8 [56320/60032 (94%)]\tLoss: 0.008514\n",
            "Train Epoch: 8 [56960/60032 (95%)]\tLoss: 0.041729\n",
            "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.075201\n",
            "Train Epoch: 8 [58240/60032 (97%)]\tLoss: 0.011030\n",
            "Train Epoch: 8 [58880/60032 (98%)]\tLoss: 0.110500\n",
            "Train Epoch: 8 [59520/60032 (99%)]\tLoss: 0.015867\n",
            "\n",
            "Test set: Average loss: 0.0392, Accuracy: 9874/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.122474\n",
            "Train Epoch: 9 [640/60032 (1%)]\tLoss: 0.020601\n",
            "Train Epoch: 9 [1280/60032 (2%)]\tLoss: 0.011366\n",
            "Train Epoch: 9 [1920/60032 (3%)]\tLoss: 0.022259\n",
            "Train Epoch: 9 [2560/60032 (4%)]\tLoss: 0.053344\n",
            "Train Epoch: 9 [3200/60032 (5%)]\tLoss: 0.025076\n",
            "Train Epoch: 9 [3840/60032 (6%)]\tLoss: 0.013174\n",
            "Train Epoch: 9 [4480/60032 (7%)]\tLoss: 0.012291\n",
            "Train Epoch: 9 [5120/60032 (9%)]\tLoss: 0.125409\n",
            "Train Epoch: 9 [5760/60032 (10%)]\tLoss: 0.019490\n",
            "Train Epoch: 9 [6400/60032 (11%)]\tLoss: 0.018156\n",
            "Train Epoch: 9 [7040/60032 (12%)]\tLoss: 0.020821\n",
            "Train Epoch: 9 [7680/60032 (13%)]\tLoss: 0.128987\n",
            "Train Epoch: 9 [8320/60032 (14%)]\tLoss: 0.016785\n",
            "Train Epoch: 9 [8960/60032 (15%)]\tLoss: 0.041369\n",
            "Train Epoch: 9 [9600/60032 (16%)]\tLoss: 0.083387\n",
            "Train Epoch: 9 [10240/60032 (17%)]\tLoss: 0.046702\n",
            "Train Epoch: 9 [10880/60032 (18%)]\tLoss: 0.067936\n",
            "Train Epoch: 9 [11520/60032 (19%)]\tLoss: 0.004627\n",
            "Train Epoch: 9 [12160/60032 (20%)]\tLoss: 0.007573\n",
            "Train Epoch: 9 [12800/60032 (21%)]\tLoss: 0.010968\n",
            "Train Epoch: 9 [13440/60032 (22%)]\tLoss: 0.012924\n",
            "Train Epoch: 9 [14080/60032 (23%)]\tLoss: 0.077692\n",
            "Train Epoch: 9 [14720/60032 (25%)]\tLoss: 0.109353\n",
            "Train Epoch: 9 [15360/60032 (26%)]\tLoss: 0.048015\n",
            "Train Epoch: 9 [16000/60032 (27%)]\tLoss: 0.055696\n",
            "Train Epoch: 9 [16640/60032 (28%)]\tLoss: 0.056478\n",
            "Train Epoch: 9 [17280/60032 (29%)]\tLoss: 0.066137\n",
            "Train Epoch: 9 [17920/60032 (30%)]\tLoss: 0.042421\n",
            "Train Epoch: 9 [18560/60032 (31%)]\tLoss: 0.036250\n",
            "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.010536\n",
            "Train Epoch: 9 [19840/60032 (33%)]\tLoss: 0.004409\n",
            "Train Epoch: 9 [20480/60032 (34%)]\tLoss: 0.094395\n",
            "Train Epoch: 9 [21120/60032 (35%)]\tLoss: 0.065235\n",
            "Train Epoch: 9 [21760/60032 (36%)]\tLoss: 0.008797\n",
            "Train Epoch: 9 [22400/60032 (37%)]\tLoss: 0.063747\n",
            "Train Epoch: 9 [23040/60032 (38%)]\tLoss: 0.023818\n",
            "Train Epoch: 9 [23680/60032 (39%)]\tLoss: 0.010220\n",
            "Train Epoch: 9 [24320/60032 (41%)]\tLoss: 0.013939\n",
            "Train Epoch: 9 [24960/60032 (42%)]\tLoss: 0.002819\n",
            "Train Epoch: 9 [25600/60032 (43%)]\tLoss: 0.015462\n",
            "Train Epoch: 9 [26240/60032 (44%)]\tLoss: 0.049887\n",
            "Train Epoch: 9 [26880/60032 (45%)]\tLoss: 0.055040\n",
            "Train Epoch: 9 [27520/60032 (46%)]\tLoss: 0.046757\n",
            "Train Epoch: 9 [28160/60032 (47%)]\tLoss: 0.009853\n",
            "Train Epoch: 9 [28800/60032 (48%)]\tLoss: 0.045440\n",
            "Train Epoch: 9 [29440/60032 (49%)]\tLoss: 0.034183\n",
            "Train Epoch: 9 [30080/60032 (50%)]\tLoss: 0.004226\n",
            "Train Epoch: 9 [30720/60032 (51%)]\tLoss: 0.043186\n",
            "Train Epoch: 9 [31360/60032 (52%)]\tLoss: 0.033284\n",
            "Train Epoch: 9 [32000/60032 (53%)]\tLoss: 0.010189\n",
            "Train Epoch: 9 [32640/60032 (54%)]\tLoss: 0.020675\n",
            "Train Epoch: 9 [33280/60032 (55%)]\tLoss: 0.031353\n",
            "Train Epoch: 9 [33920/60032 (57%)]\tLoss: 0.021329\n",
            "Train Epoch: 9 [34560/60032 (58%)]\tLoss: 0.012174\n",
            "Train Epoch: 9 [35200/60032 (59%)]\tLoss: 0.117923\n",
            "Train Epoch: 9 [35840/60032 (60%)]\tLoss: 0.050959\n",
            "Train Epoch: 9 [36480/60032 (61%)]\tLoss: 0.034033\n",
            "Train Epoch: 9 [37120/60032 (62%)]\tLoss: 0.016778\n",
            "Train Epoch: 9 [37760/60032 (63%)]\tLoss: 0.025700\n",
            "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.106904\n",
            "Train Epoch: 9 [39040/60032 (65%)]\tLoss: 0.004782\n",
            "Train Epoch: 9 [39680/60032 (66%)]\tLoss: 0.040262\n",
            "Train Epoch: 9 [40320/60032 (67%)]\tLoss: 0.053806\n",
            "Train Epoch: 9 [40960/60032 (68%)]\tLoss: 0.036670\n",
            "Train Epoch: 9 [41600/60032 (69%)]\tLoss: 0.001931\n",
            "Train Epoch: 9 [42240/60032 (70%)]\tLoss: 0.079998\n",
            "Train Epoch: 9 [42880/60032 (71%)]\tLoss: 0.005327\n",
            "Train Epoch: 9 [43520/60032 (72%)]\tLoss: 0.052708\n",
            "Train Epoch: 9 [44160/60032 (74%)]\tLoss: 0.044313\n",
            "Train Epoch: 9 [44800/60032 (75%)]\tLoss: 0.032287\n",
            "Train Epoch: 9 [45440/60032 (76%)]\tLoss: 0.057284\n",
            "Train Epoch: 9 [46080/60032 (77%)]\tLoss: 0.017856\n",
            "Train Epoch: 9 [46720/60032 (78%)]\tLoss: 0.061742\n",
            "Train Epoch: 9 [47360/60032 (79%)]\tLoss: 0.159292\n",
            "Train Epoch: 9 [48000/60032 (80%)]\tLoss: 0.040139\n",
            "Train Epoch: 9 [48640/60032 (81%)]\tLoss: 0.039376\n",
            "Train Epoch: 9 [49280/60032 (82%)]\tLoss: 0.013785\n",
            "Train Epoch: 9 [49920/60032 (83%)]\tLoss: 0.045515\n",
            "Train Epoch: 9 [50560/60032 (84%)]\tLoss: 0.010760\n",
            "Train Epoch: 9 [51200/60032 (85%)]\tLoss: 0.021212\n",
            "Train Epoch: 9 [51840/60032 (86%)]\tLoss: 0.016506\n",
            "Train Epoch: 9 [52480/60032 (87%)]\tLoss: 0.027196\n",
            "Train Epoch: 9 [53120/60032 (88%)]\tLoss: 0.172877\n",
            "Train Epoch: 9 [53760/60032 (90%)]\tLoss: 0.004599\n",
            "Train Epoch: 9 [54400/60032 (91%)]\tLoss: 0.014329\n",
            "Train Epoch: 9 [55040/60032 (92%)]\tLoss: 0.013661\n",
            "Train Epoch: 9 [55680/60032 (93%)]\tLoss: 0.041459\n",
            "Train Epoch: 9 [56320/60032 (94%)]\tLoss: 0.014859\n",
            "Train Epoch: 9 [56960/60032 (95%)]\tLoss: 0.049032\n",
            "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.035813\n",
            "Train Epoch: 9 [58240/60032 (97%)]\tLoss: 0.032728\n",
            "Train Epoch: 9 [58880/60032 (98%)]\tLoss: 0.077139\n",
            "Train Epoch: 9 [59520/60032 (99%)]\tLoss: 0.072986\n",
            "\n",
            "Test set: Average loss: 0.0427, Accuracy: 9854/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.011451\n",
            "Train Epoch: 10 [640/60032 (1%)]\tLoss: 0.021752\n",
            "Train Epoch: 10 [1280/60032 (2%)]\tLoss: 0.064347\n",
            "Train Epoch: 10 [1920/60032 (3%)]\tLoss: 0.011675\n",
            "Train Epoch: 10 [2560/60032 (4%)]\tLoss: 0.002058\n",
            "Train Epoch: 10 [3200/60032 (5%)]\tLoss: 0.042565\n",
            "Train Epoch: 10 [3840/60032 (6%)]\tLoss: 0.007678\n",
            "Train Epoch: 10 [4480/60032 (7%)]\tLoss: 0.002436\n",
            "Train Epoch: 10 [5120/60032 (9%)]\tLoss: 0.013937\n",
            "Train Epoch: 10 [5760/60032 (10%)]\tLoss: 0.008509\n",
            "Train Epoch: 10 [6400/60032 (11%)]\tLoss: 0.022384\n",
            "Train Epoch: 10 [7040/60032 (12%)]\tLoss: 0.057672\n",
            "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.022134\n",
            "Train Epoch: 10 [8320/60032 (14%)]\tLoss: 0.005648\n",
            "Train Epoch: 10 [8960/60032 (15%)]\tLoss: 0.006835\n",
            "Train Epoch: 10 [9600/60032 (16%)]\tLoss: 0.007762\n",
            "Train Epoch: 10 [10240/60032 (17%)]\tLoss: 0.040670\n",
            "Train Epoch: 10 [10880/60032 (18%)]\tLoss: 0.020584\n",
            "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.033016\n",
            "Train Epoch: 10 [12160/60032 (20%)]\tLoss: 0.030611\n",
            "Train Epoch: 10 [12800/60032 (21%)]\tLoss: 0.024301\n",
            "Train Epoch: 10 [13440/60032 (22%)]\tLoss: 0.019356\n",
            "Train Epoch: 10 [14080/60032 (23%)]\tLoss: 0.028614\n",
            "Train Epoch: 10 [14720/60032 (25%)]\tLoss: 0.120555\n",
            "Train Epoch: 10 [15360/60032 (26%)]\tLoss: 0.005400\n",
            "Train Epoch: 10 [16000/60032 (27%)]\tLoss: 0.102693\n",
            "Train Epoch: 10 [16640/60032 (28%)]\tLoss: 0.031609\n",
            "Train Epoch: 10 [17280/60032 (29%)]\tLoss: 0.012520\n",
            "Train Epoch: 10 [17920/60032 (30%)]\tLoss: 0.026162\n",
            "Train Epoch: 10 [18560/60032 (31%)]\tLoss: 0.045229\n",
            "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.025283\n",
            "Train Epoch: 10 [19840/60032 (33%)]\tLoss: 0.013817\n",
            "Train Epoch: 10 [20480/60032 (34%)]\tLoss: 0.181028\n",
            "Train Epoch: 10 [21120/60032 (35%)]\tLoss: 0.007453\n",
            "Train Epoch: 10 [21760/60032 (36%)]\tLoss: 0.024282\n",
            "Train Epoch: 10 [22400/60032 (37%)]\tLoss: 0.028244\n",
            "Train Epoch: 10 [23040/60032 (38%)]\tLoss: 0.018918\n",
            "Train Epoch: 10 [23680/60032 (39%)]\tLoss: 0.013389\n",
            "Train Epoch: 10 [24320/60032 (41%)]\tLoss: 0.007258\n",
            "Train Epoch: 10 [24960/60032 (42%)]\tLoss: 0.021336\n",
            "Train Epoch: 10 [25600/60032 (43%)]\tLoss: 0.097925\n",
            "Train Epoch: 10 [26240/60032 (44%)]\tLoss: 0.009022\n",
            "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.019054\n",
            "Train Epoch: 10 [27520/60032 (46%)]\tLoss: 0.038322\n",
            "Train Epoch: 10 [28160/60032 (47%)]\tLoss: 0.123178\n",
            "Train Epoch: 10 [28800/60032 (48%)]\tLoss: 0.063764\n",
            "Train Epoch: 10 [29440/60032 (49%)]\tLoss: 0.046642\n",
            "Train Epoch: 10 [30080/60032 (50%)]\tLoss: 0.058700\n",
            "Train Epoch: 10 [30720/60032 (51%)]\tLoss: 0.031819\n",
            "Train Epoch: 10 [31360/60032 (52%)]\tLoss: 0.081197\n",
            "Train Epoch: 10 [32000/60032 (53%)]\tLoss: 0.001933\n",
            "Train Epoch: 10 [32640/60032 (54%)]\tLoss: 0.021334\n",
            "Train Epoch: 10 [33280/60032 (55%)]\tLoss: 0.034316\n",
            "Train Epoch: 10 [33920/60032 (57%)]\tLoss: 0.008230\n",
            "Train Epoch: 10 [34560/60032 (58%)]\tLoss: 0.036735\n",
            "Train Epoch: 10 [35200/60032 (59%)]\tLoss: 0.020100\n",
            "Train Epoch: 10 [35840/60032 (60%)]\tLoss: 0.082057\n",
            "Train Epoch: 10 [36480/60032 (61%)]\tLoss: 0.011593\n",
            "Train Epoch: 10 [37120/60032 (62%)]\tLoss: 0.061417\n",
            "Train Epoch: 10 [37760/60032 (63%)]\tLoss: 0.003418\n",
            "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.005231\n",
            "Train Epoch: 10 [39040/60032 (65%)]\tLoss: 0.010868\n",
            "Train Epoch: 10 [39680/60032 (66%)]\tLoss: 0.023426\n",
            "Train Epoch: 10 [40320/60032 (67%)]\tLoss: 0.034223\n",
            "Train Epoch: 10 [40960/60032 (68%)]\tLoss: 0.022992\n",
            "Train Epoch: 10 [41600/60032 (69%)]\tLoss: 0.070264\n",
            "Train Epoch: 10 [42240/60032 (70%)]\tLoss: 0.026406\n",
            "Train Epoch: 10 [42880/60032 (71%)]\tLoss: 0.011131\n",
            "Train Epoch: 10 [43520/60032 (72%)]\tLoss: 0.078265\n",
            "Train Epoch: 10 [44160/60032 (74%)]\tLoss: 0.044495\n",
            "Train Epoch: 10 [44800/60032 (75%)]\tLoss: 0.019916\n",
            "Train Epoch: 10 [45440/60032 (76%)]\tLoss: 0.015379\n",
            "Train Epoch: 10 [46080/60032 (77%)]\tLoss: 0.003633\n",
            "Train Epoch: 10 [46720/60032 (78%)]\tLoss: 0.010158\n",
            "Train Epoch: 10 [47360/60032 (79%)]\tLoss: 0.053545\n",
            "Train Epoch: 10 [48000/60032 (80%)]\tLoss: 0.028536\n",
            "Train Epoch: 10 [48640/60032 (81%)]\tLoss: 0.016899\n",
            "Train Epoch: 10 [49280/60032 (82%)]\tLoss: 0.034963\n",
            "Train Epoch: 10 [49920/60032 (83%)]\tLoss: 0.027169\n",
            "Train Epoch: 10 [50560/60032 (84%)]\tLoss: 0.020548\n",
            "Train Epoch: 10 [51200/60032 (85%)]\tLoss: 0.021499\n",
            "Train Epoch: 10 [51840/60032 (86%)]\tLoss: 0.009159\n",
            "Train Epoch: 10 [52480/60032 (87%)]\tLoss: 0.004679\n",
            "Train Epoch: 10 [53120/60032 (88%)]\tLoss: 0.240532\n",
            "Train Epoch: 10 [53760/60032 (90%)]\tLoss: 0.001164\n",
            "Train Epoch: 10 [54400/60032 (91%)]\tLoss: 0.008152\n",
            "Train Epoch: 10 [55040/60032 (92%)]\tLoss: 0.082201\n",
            "Train Epoch: 10 [55680/60032 (93%)]\tLoss: 0.017729\n",
            "Train Epoch: 10 [56320/60032 (94%)]\tLoss: 0.053357\n",
            "Train Epoch: 10 [56960/60032 (95%)]\tLoss: 0.009189\n",
            "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.031754\n",
            "Train Epoch: 10 [58240/60032 (97%)]\tLoss: 0.023230\n",
            "Train Epoch: 10 [58880/60032 (98%)]\tLoss: 0.011324\n",
            "Train Epoch: 10 [59520/60032 (99%)]\tLoss: 0.152542\n",
            "\n",
            "Test set: Average loss: 0.0330, Accuracy: 9889/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}